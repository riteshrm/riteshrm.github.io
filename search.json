[
  {
    "objectID": "posts/Quantization in Depth/index.html",
    "href": "posts/Quantization in Depth/index.html",
    "title": "Quantization in Depth",
    "section": "",
    "text": "This is completely based on Quantization in Depth\nFor the code part, you can checkout this link\n\n\nQuantize and De-quantize a tensor\n\nAdvantages of Quantization\n\nSmaller model\nSpeed gains\n\nMemory bandwidth\nFaster operations\n\nGEMM: General Matrix Multiply(matrix to matrix multiplication)\nGEMV: General Matrix Vector Multiplication (matrix to vector multiplication)\n\n\n\nChallenges of Quantization\n\nQuantization error\nRetraining (Quantization Aware Training)\nLimited Hardware support\nCalibration dataset needed\npacking/unpacking\n\ngetting q:-\n\nr = s(q-z) q = int(round(r/s+z))\n\n\n\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n#Helper functions to visualize\ndef plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=None):\n    \"\"\"\n    Plot a heatmap of tensors using seaborn\n    \"\"\"\n    sns.heatmap(tensor.cpu().numpy(), ax=ax, vmin=vmin, vmax=vmax, cmap=cmap, annot=True, fmt=\".2f\", cbar=False)\n    ax.set_title(title)\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n\n\ndef plot_quantization_errors(original_tensor, quantized_tensor, dequantized_tensor, dtype = torch.int8, n_bits = 8):\n    \"\"\"\n    A method that plots 4 matrices, the original tensor, the quantized tensor\n    the de-quantized tensor and the error tensor.\n    \"\"\"\n    # Get a figure of 4 plots\n    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n\n    # Plot the first matrix\n    plot_matrix(original_tensor, axes[0], 'Original Tensor', cmap=ListedColormap(['white']))\n\n    # Get the quantization range and plot the quantized tensor\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    plot_matrix(quantized_tensor, axes[1], f'{n_bits}-bit Linear Quantized Tensor', vmin=q_min, vmax=q_max, cmap='coolwarm')\n\n    # Plot the de-quantized tensors\n    plot_matrix(dequantized_tensor, axes[2], 'Dequantized Tensor', cmap='coolwarm')\n\n    # Get the quantization errors\n    q_error_tensor = abs(original_tensor - dequantized_tensor)\n    plot_matrix(q_error_tensor, axes[3], 'Quantization Error Tensor', cmap=ListedColormap(['white']))\n\n    fig.tight_layout()\n    plt.show()\n\ndef linear_q_with_scale_and_zero_point(\n    tensor, scale, zero_point, dtype = torch.int8):\n\n    scaled_and_shifted_tensor = tensor / scale + zero_point\n\n    rounded_tensor = torch.round(scaled_and_shifted_tensor)\n\n    q_min = torch.iinfo(dtype).min\n    q_max = torch.iinfo(dtype).max\n\n    q_tensor = rounded_tensor.clamp(q_min,q_max).to(dtype)\n    \n    return q_tensor\n\ntest_tensor=torch.tensor(\n    [[191.6, -13.5, 728.6],\n     [92.14, 295.5,  -184],\n     [0,     684.6, 245.5]])\n\nscale = 3.5\nzero_point = -70\n\nquantized_tensor = linear_q_with_scale_and_zero_point(\n    test_tensor, scale, zero_point)\n\ndef linear_dequantization(quantized_tensor, scale, zero_point):\n    return scale * (quantized_tensor.float() - zero_point)\n\ndequantized_tensor = linear_dequantization(\n    quantized_tensor, scale, zero_point)\n\nplot_quantization_errors(test_tensor, quantized_tensor,\n                         dequantized_tensor)\n\n\n\n\n\n\n\n\n\n\nGet the Scale and Zero-Point\n\ns = (r_max-r_min)[current_tensor_range]/(q_max-q_min)[datatype_range]\nz = int(round(q_min - r_min/s))\nz and quantized tensor are of the same type\nz is an integer because it represent zero(in the original ‘r’ range) with an integer in the quantized ‘q’ range\nif z goes out of range:-\n\nz &lt; q_min:-\n\nz = q_min\n\nz &gt; q_max:-\n\nz = q_max\n\n\n\n\nimport torch\n\ndef get_q_scale_and_zero_point(tensor, dtype=torch.int8):\n    \n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    r_min, r_max = tensor.min().item(), tensor.max().item()\n\n    scale = (r_max - r_min) / (q_max - q_min)\n\n    zero_point = q_min - (r_min / scale)\n\n    # clip the zero_point to fall in [quantized_min, quantized_max]\n    if zero_point &lt; q_min:\n        zero_point = q_min\n    elif zero_point &gt; q_max:\n        zero_point = q_max\n    else:\n        # round and cast to int\n        zero_point = int(round(zero_point))\n    \n    return scale, zero_point\n\n\n\nSymmetric vs Asymmetrci Mode\n\nAssymetric Mode:-\n\nmap [r_max, r_min] to [q_max, q_min]\nThis is what we have implemnted above\n\nSymmetric Mode:-\n\nmap [-r_max, r_max] to [-q_max, q_max]\n\nwhere r_max = max(|tensor|)\n\n\n\nWe don’t need to use zero point(z=0). this happens because the floating point range and the quantized range are symmetric with respect to zero\n\n\n\n\n\nHence, we can simplify the equation to:-\n\nq = int(round(r/s))\ns = r_max/q_max\n\n\nimport torch\n\ndef get_q_scale_symmetric(tensor, dtype=torch.int8):\n    r_max = tensor.abs().max().item()\n    q_max = torch.iinfo(dtype).max\n\n    # return the scale\n    return r_max/q_max\n\ndef linear_q_symmetric(tensor, dtype=torch.int8):\n    scale = get_q_scale_symmetric(tensor)\n    \n    quantized_tensor = linear_q_with_scale_and_zero_point(tensor,\n                                                     scale=scale,\n                   # in symmetric quantization zero point is = 0    \n                                                    zero_point=0,\n                                                      dtype=dtype)\n    \n    return quantized_tensor, scale\n\nTrade-off\n\nUtilization of quantized range:\n\nwhen using asymmetric quantization, the quantized range is fully utilized\nWhen symmetric mode, if the float range is biased towards one side, this will result in a quantized range where a part of the range is dedicated to values that we’ll never see.(e.g ReLU where the output is positive)\n\nSimplicity:\n\nSymmetric mode is much simpler compared to asymmetric mode.\n\nMemory: We don’t have to store zero-point for symmetric quantization\nWe use symmetric quantization for 8-bit, but as we go for lower bits such as 2 or 4 bits, we use asyyemtric quantization\n\n\n\nFiner Granularity for more Precision\n\nDifferent granularities\n\nper tensor\nper channel (along an axis)\nper group (group n elements together)\n\nThe more granular quantization is the more precise it will be.\n\n\n\nPer Channel Quantization\n\nwe usually use per channel quantization in int8\n\n\ndef linear_q_symmetric_per_channel(r_tensor, dim, dtype=torch.int8):\n    \n    output_dim = r_tensor.shape[dim]\n    # store the scales\n    scale = torch.zeros(output_dim)\n\n    for index in range(output_dim):\n        sub_tensor = r_tensor.select(dim, index)\n        scale[index] = get_q_scale_symmetric(sub_tensor, dtype=dtype)\n\n    # reshape the scale\n    scale_shape = [1] * r_tensor.dim()\n    scale_shape[dim] = -1\n    scale = scale.view(scale_shape)\n    quantized_tensor = linear_q_with_scale_and_zero_point(\n        r_tensor, scale=scale, zero_point=0, dtype=dtype)\n   \n    return quantized_tensor, scale\n\n\n\nPer Group Quantization\n\nGroup n(e.g. 32, 64, 128) elements together and quantize\nPer group quantization can require a lot of memory\n\nLet’s say we want to quantize a tensor in 4-bit and we choose group_size=32, symmetric mode(z=0), and we store the scales in FP16\nIt means that we actually quantizing the tensor in 4.5 bits since we have:\n\n4 bit(each element is stored in 4 bit)\n16/32 bit (scale in 16 bits for every 32 elements)\n\n\n\n\ndef linear_q_symmetric_per_group(tensor, group_size,\n                                 dtype=torch.int8):\n    \n    t_shape = tensor.shape\n    assert t_shape[1] % group_size == 0\n    assert tensor.dim() == 2\n    \n    tensor = tensor.view(-1, group_size)\n    \n    quantized_tensor, scale = linear_q_symmetric_per_channel(\n                                tensor, dim=0, dtype=dtype)\n    \n    quantized_tensor = quantized_tensor.view(t_shape)\n    \n    return quantized_tensor, scale\n\ndef linear_dequantization_per_group(quantized_tensor, scale, \n                                    group_size):\n    \n    q_shape = quantized_tensor.shape\n    quantized_tensor = quantized_tensor.view(-1, group_size)\n    \n    dequantized_tensor = linear_dequantization(quantized_tensor, \n                                               scale, 0)\n    \n    dequantized_tensor = dequantized_tensor.view(q_shape)\n    \n    return dequantized_tensor\n\n\n\nQuantizing Weights and Activations for Inference\n\nDepending on what we quantize, the storage and the computation are not the same.\nW8A32\n\nIf weights are quantized but not the activations, then computation is done floating point (FP16,FP32, BF16)\nWe need to dequantize the weights to perform the floating point computation (cast to float32)\n\nW8A8\n\nBoth are quantized\nComputation is integer based but not supported by all hardware\n\n\n\n\nCustom Build an 8-Bit Quantizer\n\n#W8A16LinearLayer\ndef w8_a16_forward(weight, input, scales, bias=None):\n    \n    casted_weights = weight.to(input.dtype)\n    output = F.linear(input, casted_weights) * scales\n    \n    if bias is not None:\n        output = output + bias\n      \n    return output\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass W8A16LinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, \n                 bias=True, dtype=torch.float32):\n        super().__init__()\n        \n        self.int8_weights = nn.Parameter(torch.Tensor([0, 1]\n                                     ).to(dtype=torch.int8))\n\ntry:\n    \n    W8A16LinearLayer(1, 1)\n    \nexcept Exception as error:\n    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")\n\n RuntimeError :  Only Tensors of floating point and complex dtype can require gradients \n\n\n\nWhen we create nn.Parameters, pytorch expects that parameter where it’s able to compute gradients on it.\nThe issue is that with PyTorch, you can’t explicitly compute gradients on INT8 tensors.\nSo above code snippet will give an error saying that only tensors of floating point and complex dtype can require gradients.\nSo the right approach to save INT8 weights is instead of saving attributes as being an endless parameter, is to call the method called register buffer.\nThis way instead of storing a parameter, we just store a buffer, meaning we don’t need to compute gradients on the tensor.\nYou can initialize it with whatever dtype you want.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass W8A16LinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, \n                 bias=True, dtype=torch.float32):\n        super().__init__()\n        \n        \n        self.register_buffer(\n            \"int8_weights\",\n            torch.randint(\n                -128, 127, (out_features, in_features), dtype=torch.int8\n            )\n        )\n        \n        self.register_buffer(\"scales\", \n                             torch.randn((out_features), dtype=dtype)) # We are intereseted in inference only\n        \n        if bias:\n            self.register_buffer(\"bias\", \n                                 torch.randn((1, out_features), \n                                             dtype=dtype))\n        \n        else:\n            self.bias = None\n    def forward(self, input):\n        return w8_a16_forward(self.int8_weights, \n                              input, self.scales, self.bias)\n\nQuantize a Base Model\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass W8A16LinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, \n                 bias=True, dtype=torch.float32):\n        super().__init__()\n        \n        \n        self.register_buffer(\n            \"int8_weights\",\n            torch.randint(\n                -128, 127, (out_features, in_features), dtype=torch.int8\n            )\n        )\n        \n        self.register_buffer(\"scales\", \n                             torch.randn((out_features), dtype=dtype))\n        \n        if bias:\n            self.register_buffer(\"bias\", \n                                 torch.randn((1, out_features), \n                                             dtype=dtype))\n        \n        else:\n            self.bias = None\n\n    def quantize(self, weights):\n        w_fp32 = weights.clone().to(torch.float32)\n\n        scales = w_fp32.abs().max(dim=-1).values / 127\n        scales = scales.to(weights.dtype)\n\n        int8_weights = torch.round(weights\n                        /scales.unsqueeze(1)).to(torch.int8)\n\n        self.int8_weights = int8_weights\n        self.scales = scales\n    \n    def forward(self, input):\n        return w8_a16_forward(self.int8_weights, \n                              input, self.scales, self.bias)\n\nmodule = W8A16LinearLayer(4, 8)\nprint(\"Weights before:\\n\" , module.int8_weights)\nrandom_matrix = torch.randn((4, 8), dtype=torch.bfloat16)\nmodule.quantize(random_matrix)\nprint(\"Weights After:\\n\" , module.int8_weights)\nprint(\"Average quantiation error:-\",(random_matrix - module.int8_weights \n * module.scales.unsqueeze(1)).abs().mean())\n\nWeights before:\n tensor([[ -77,   65,  -65,  -97],\n        [  77, -109,  -82,   11],\n        [  68,  -43,  -80,  -52],\n        [  90,  -73,   82, -110],\n        [ 106,  -43,  -73,  116],\n        [ -75,  -61,  -19,  -31],\n        [ -69,  -14,  -52,   19],\n        [ -42,  -91,   76,   48]], dtype=torch.int8)\nWeights After:\n tensor([[   7, -127,  -74, -111,   20,  -90, -100,   80],\n        [ 127,  108, -104,   75,  -48,    0,   41,  -41],\n        [  39,  -91,   28,   53,   77,   48,   25, -128],\n        [ -66, -118, -120,   38,  -72,   18,  127,  108]], dtype=torch.int8)\nAverage quantiation error:- tensor(0.0928, dtype=torch.bfloat16)\n\n\n\n\nReplace PyTorch layers with Quantized Layers\n\nFor language models, it better to not quantize the last layer.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef replace_linear_with_target(module, \n                               target_class, module_name_to_exclude):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear) and not \\\n          any([x == name for x in module_name_to_exclude]):\n            old_bias = child.bias\n\n            new_module = target_class(child.in_features, \n                                      child.out_features, \n                                      old_bias is not None, \n                                      child.weight.dtype)\n            setattr(module, name, new_module)\n            if old_bias is not None:\n              getattr(module, name).bias = old_bias\n        else:\n            # Recursively call the function for nested modules\n            replace_linear_with_target(\n                child, target_class, module_name_to_exclude)\n\n\ndef replace_linear_with_target_and_quantize(module, \n                               target_class, module_name_to_exclude):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear) and not \\\n        any([x == name for x in module_name_to_exclude]):\n            old_bias = child.bias\n            old_weight = child.weight\n\n            new_module = target_class(child.in_features, \n                                      child.out_features, \n                                      old_bias is not None, \n                                      child.weight.dtype)\n            setattr(module, name, new_module) # current module is replaced by new_module\n\n            getattr(module, name).quantize(old_weight)\n            \n            if old_bias is not None:\n              getattr(module, name).bias = old_bias\n        else:\n            # Recursively call the function for nested modules\n            replace_linear_with_target_and_quantize(child, \n                     target_class, module_name_to_exclude)\n\nclass DummyModel(torch.nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.emb = torch.nn.Embedding(1, 1)\n    # Try with bias\n    self.linear_1 = nn.Linear(1, 1)\n    # Try without bias\n    self.linear_2 = nn.Linear(1, 1, bias=False)\n    # Lm prediction head\n    self.lm_head = nn.Linear(1, 1, bias=False)\n\nmodel_1 = DummyModel()\nmodel_2 = DummyModel()\nreplace_linear_with_target_and_quantize(model_1, W8A16LinearLayer, [\"lm_head\"])\nprint(\"model_1\",model_1)\n\nreplace_linear_with_target_and_quantize(model_2, W8A16LinearLayer, [])\nprint(\"model_2\",model_2)\n\nmodel_1 DummyModel(\n  (emb): Embedding(1, 1)\n  (linear_1): W8A16LinearLayer()\n  (linear_2): W8A16LinearLayer()\n  (lm_head): Linear(in_features=1, out_features=1, bias=False)\n)\nmodel_2 DummyModel(\n  (emb): Embedding(1, 1)\n  (linear_1): W8A16LinearLayer()\n  (linear_2): W8A16LinearLayer()\n  (lm_head): W8A16LinearLayer()\n)\n\n\n\n\nQuantize any Open Source PyTorch Model\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_id = \"Salesforce/codegen-350M-mono\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, \n                                    torch_dtype=torch.bfloat16, \n                                             low_cpu_mem_usage=True)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nprint(pipe(\"def hello_world():\", max_new_tokens=20, do_sample=False)[0][\"generated_text\"])\nprint(\"Model before:\\n\\n\", model)\nreplace_linear_with_target_and_quantize(model, \n                                        W8A16LinearLayer, [\"lm_head\"])\n\nprint(\"Model after:\\n\\n\", model)\nprint(pipe(\"def hello_world():\", max_new_tokens=20, \n           do_sample=False)[0][\"generated_text\"])\n\n\n\n\nC:\\Users\\rites\\miniconda3\\envs\\quarto\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rites\\.cache\\huggingface\\hub\\models--Salesforce--codegen-350M-mono. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\nTo support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n  warnings.warn(message)\n\n\n\n\n\nSome weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevice set to use cuda:0\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\ndef hello_world():\n    print(\"Hello World\")\n\nhello_world()\n\n# 파\nModel before:\n\n CodeGenForCausalLM(\n  (transformer): CodeGenModel(\n    (wte): Embedding(51200, 1024)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-19): 20 x CodeGenBlock(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): CodeGenAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n        )\n        (mlp): CodeGenMLP(\n          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n)\n\n\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n\n\nModel after:\n\n CodeGenForCausalLM(\n  (transformer): CodeGenModel(\n    (wte): Embedding(51200, 1024)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-19): 20 x CodeGenBlock(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): CodeGenAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (qkv_proj): W8A16LinearLayer()\n          (out_proj): W8A16LinearLayer()\n        )\n        (mlp): CodeGenMLP(\n          (fc_in): W8A16LinearLayer()\n          (fc_out): W8A16LinearLayer()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n)\ndef hello_world():\n    print(\"Hello World\")\n\n# hello_world()\n\n# def hello_\n\n\n\nAbove code snippet modifies the model inplace\nAlso don’t try to change the lm_head otherwise it will not give the desired results\nAll the rounding errors can sum up once you start generating a lot of tokens, until may be all of these errors get super large so that it affects the model’s performance\n\n\n\nLoad your Quantized Weights from HuggingFace Hub\n\nThe idea is to quantize weights on bigger instance and then push it back to huggingface. So that we don’t have to load and quantize again and again.\nThen use meta device from pytorch to load the skeleton of the model instead of loading the whole model itself.\nReplace the original layers with the quantized layers\nLoad the quantized weights from huggingfacehub\n\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"facebook/opt-125m\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nreplace_linear_with_target_and_quantize(model, \n                             W8A16LinearLayer, \n                                   [\"lm_head\"])\n\nmodel\n\nquantized_state_dict = model.state_dict()\ntorch.save(quantized_state_dict, r\"C:\\wsl\\random\\models\\quantized_state_dict.pth\")\n\nHow to upload on HF\nfrom huggingface_hub import HfApi, create_repo\nYOUR_HF_USERNAME = “” your_repo_id = f”{YOUR_HF_USERNAME}/opt-125m-quantized-dlai”\napi = HfApi()\ncreate_repo(your_repo_id)\napi.upload_file( path_or_fileobj=“quantized_state_dict.pth”, path_in_repo=“quantized_state_dict.pth”, repo_id=your_repo_id )\n\nimport torch\nfrom transformers import OPTForCausalLM, AutoTokenizer, AutoConfig\n\nmodel_id = \"facebook/opt-125m\"\nconfig = AutoConfig.from_pretrained(model_id)\n\nwith torch.device(\"meta\"):\n  model = OPTForCausalLM(config)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nfor param in model.parameters():\n  print(param)\n  break\n\n\n\n\nParameter containing:\ntensor(..., device='meta', size=(50272, 768), requires_grad=True)\n\n\n\nmodel\n\nOPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0-11): 12 x OPTDecoderLayer(\n          (self_attn): OPTSdpaAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)\n\n\n\nreplace_linear_with_target(model, W8A16LinearLayer, [\"lm_head\"])\nmodel\n\nOPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0-11): 12 x OPTDecoderLayer(\n          (self_attn): OPTSdpaAttention(\n            (k_proj): W8A16LinearLayer()\n            (v_proj): W8A16LinearLayer()\n            (q_proj): W8A16LinearLayer()\n            (out_proj): W8A16LinearLayer()\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): W8A16LinearLayer()\n          (fc2): W8A16LinearLayer()\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)\n\n\nIf loading from HF\nfrom huggingface_hub import hf_hub_download\nstate_dict_cache_path = hf_hub_download( “ybelkada/opt-125m-quantized-dlai”, “quantized_state_dict.pth” )\n\nstate_dict = torch.load(r\"C:\\wsl\\random\\models\\quantized_state_dict.pth\")\nmodel.load_state_dict(state_dict, strict=True, assign=True)\n\nC:\\Users\\rites\\AppData\\Local\\Temp\\ipykernel_11372\\1779271493.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(r\"C:\\wsl\\random\\models\\quantized_state_dict.pth\")\n\n\n&lt;All keys matched successfully&gt;\n\n\n\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\npipe(\"Hello today I am\", max_new_tokens=40)\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\npipe(\"Hello today I am giving a course about\", max_new_tokens=10)\n\nDevice set to use cuda:0\nDevice set to use cuda:0\n\n\n[{'generated_text': 'Hello today I am giving a course about the history of the world. I am a student'}]\n\n\n\n\nWeights Packing\n\nWeights packing is important for storing quantized weights, because torch.int4 is not available as of today, so we need to store and load the weights in int8\nThis is not ideal because:\n\ntensor will occupy 8-bit per datapoint and might add a considerable overhead for large models\nThere would be no point of quantizing to 2/4 bits becuase we are still using 8-bit\n\nSo, we need to pack values\nConsider a tensor with 4 values each with 2-bit(0,1,2,3) precision but stored in 8-bit\n\ntensor = torch.tensor([1,0,3,2], dtype=torch.uint8)\n1:- 00000001\n0:- 00000000\n3:- 00000011\n2:- 00000010\n\nWe can pack all these values into a single 8-bit value as 177\n\n177:- 10110001\n\nAdavantages:-\n\nIt reflects the true memory footprint of the quantized weights Disadvantages:-\nThe unpacked tensors need to be a shape with a multiple of 8//nbits\nIt needs to unpack before performing an operation\n\n\n\n\nPacking 2-bit Weights\n\nimport torch\n\n# Example Tensor: [1, 0, 3, 2]\n    # 1 0 3 2 - 01 00 11 10\n\n    # Starting point of packed int8 Tensor\n    # [0000 0000]\n\n    ##### First Iteration Start:\n    # packed int8 Tensor State: [0000 0000]\n    # 1 = 0000 0001\n    # 0000 0001\n    # No left shifts in the First Iteration\n    # After bit-wise OR operation between 0000 0000 and 0000 0001:\n    # packed int8 Tensor State: 0000 0001\n    ##### First Iteration End\n\n    ##### Second Iteration Start:\n    # packed int8 Tensor State: [0000 0001]\n    # 0 = 0000 0000\n    # 0000 0000\n    # 2 left shifts:\n    # [0000 0000] (1 shift)-&gt; 0000 0000 (2 shift)-&gt; 0000 0000\n    # After bit-wise OR operation between 0000 0001 and 0000 0000:\n    # packed int8 Tensor State: 0000 0001\n    ##### Second Iteration End\n\n    ##### Third Iteration Start:\n    # packed int8 Tensor State: [0000 0001]\n    # 3 = 0000 0011\n    # 0000 0011\n    # 4 left shifts:\n    # [0000 0011] (1 shift)-&gt; 0000 0110 (2 shift)-&gt; 0000 1100\n    # 0000 1100 (3 shift)-&gt; 0001 1000 (4 shift)-&gt; 0011 0000\n    # After bit-wise OR operation between 0000 0001 and 0011 0000:\n    # packed int8 Tensor State: 0011 0001\n    ##### Third Iteration End\n\n    ##### Fourth Iteration Start:\n    # packed int8 Tensor State: [0011 0001]\n    # 2 = 0000 0010\n    # 0000 0010\n    # 6 left shifts:\n    # [0000 0010] (1 shift)-&gt; 0000 0100 (2 shift)-&gt; 0000 1000\n    # 0000 1000 (3 shift)-&gt; 0001 0000 (4 shift)-&gt; 0010 0000\n    # 0010 0000 (5 shift)-&gt; 0100 0000 (6 shift)-&gt; 1000 0000\n    # After bit-wise OR operation between 0011 0001 and 1000 0000:\n    # packed int8 Tensor State: 1011 0001\n    ##### Fourth Iteration End\n\n    # Final packed int8 Tensor State: [1011 0001]\n\ndef pack_weights(uint8tensor, bits):\n    if uint8tensor.shape[0] * bits % 8 != 0:\n        raise ValueError(f\"The input shape needs to be a mutiple \\\n        of {8 / bits} - got {uint8tensor.shape[0]}\")\n\n    num_values = uint8tensor.shape[0] * bits // 8\n\n    num_steps = 8 // bits\n\n    unpacked_idx = 0\n\n    packed_tensor = torch.zeros((num_values), dtype=torch.uint8)\n\n    # 1 0 3 2 - 01 00 11 10\n\n    # [0000 0000] -&gt; 0000 0001\n\n    # 0000 0001\n\n    # 0000 0000 - 0000 0000\n\n    # 0000 0011 - 0011 0000 - 0011 0001\n\n    # 1011 0001\n    \n    for i in range(num_values):\n        for j in range(num_steps):\n            packed_tensor[i] |= uint8tensor[unpacked_idx] &lt;&lt; (bits * j)\n            unpacked_idx += 1\n    return packed_tensor\nunpacked_tensor = torch.tensor([1, 0, 3, 2], \n                               dtype=torch.uint8)\npack_weights(unpacked_tensor, 2)\nunpacked_tensor = torch.tensor([1, 0, 3, 2, 3, 3, 3, 3], \n                               dtype=torch.uint8)\npack_weights(unpacked_tensor, 2)\n\ntensor([177, 255], dtype=torch.uint8)\n\n\n\n\nUnpacking 2-Bit Weights\n\nimport torch\n\n# Example Tensor: [10110001]\n    # Which was Originally: 1 0 3 2 - 01 00 11 10\n\n    # Starting point of unpacked Tensor\n    # [00000000 00000000 00000000 00000000]\n\n    ##### First Iteration Start:\n    # packed int8 Tensor: [10110001]\n    # You want to extract 01 from [101100 01]\n    # No right shifts in the First Iteration\n    # After bit-wise OR operation between 00000000 and 10110001:\n    # [10110001 00000000 00000000 00000000]\n    # unpacked Tensor state: [10110001 00000000 00000000 00000000]\n    ##### First Iteration End\n\n    ##### Second Iteration Start:\n    # packed int8 Tensor: [10110001]\n    # You want to extract 00 from [1011 00 01]\n    # 2 right shifts:\n    # [10110001] (1 shift)-&gt; 01011000 (2 shift)-&gt; 00101100\n    # After bit-wise OR operation between 00000000 and 00101100:\n    # [10110001 00101100 00000000 00000000]\n    # unpacked Tensor state: [10110001 00101100 00000000 00000000]\n    ##### Second Iteration End\n\n    ##### Third Iteration Start:\n    # packed int8 Tensor: [10110001]\n    # You want to extract 11 from [10 11 0001]\n    # 4 right shifts:\n    # [10110001] (1 shift)-&gt; 01011000 (2 shift)-&gt; 00101100\n    # 00101100 (3 shift)-&gt; 00010110 (4 shift)-&gt; 00001011\n    # After bit-wise OR operation between 00000000 and 00001011:\n    # [10110001 00101100 00001011 00000000]\n    # unpacked Tensor state: [10110001 00101100 00001011 00000000]\n    ##### Third Iteration End\n\n    ##### Fourth Iteration Start:\n    # packed int8 Tensor: [10110001]\n    # You want to extract 10 from [10 110001]\n    # 6 right shifts:\n    # [10110001] (1 shift)-&gt; 01011000 (2 shift)-&gt; 00101100\n    # 00101100 (3 shift)-&gt; 00010110 (4 shift)-&gt; 00001011\n    # 00001011 (5 shift)-&gt; 00000101 (6 shift)-&gt; 00000010\n    # After bit-wise OR operation between 00000000 and 00000010:\n    # [10110001 00101100 00001011 00000010]\n    # unpacked Tensor state: [10110001 00101100 00001011 00000010]\n    ##### Fourth Iteration End\n\n    # Last step: Perform masking (bit-wise AND operation)\n    # Mask: 00000011\n    # Bit-wise AND operation between \n    # unpacked Tensor and 00000011\n    # [10110001 00101100 00001011 00000010] &lt;- unpacked tensor\n    # [00000011 00000011 00000011 00000011] &lt;- Mask\n    # [00000001 00000000 00000011 00000010] &lt;- Result\n\n    # Final\n    # unpacked Tensor state: [00000001 00000000 00000011 00000010]\n\ndef unpack_weights(uint8tensor, bits):\n    num_values = uint8tensor.shape[0] * 8 // bits\n\n    num_steps = 8 // bits\n\n    unpacked_tensor = torch.zeros((num_values), dtype=torch.uint8)\n\n    unpacked_idx = 0\n\n    # 1 0 3 2 - 01 00 11 10\n\n    # [00000000 00000000 00000000 00000000]\n    # [10110001 00101100 00001011 00000010]\n    # [00000001 00000000 00000011 00000010]\n\n    # 10110001\n    # 00000011\n    \n    # 00000001\n\n    # 1: [10110001]\n    # 2: [00101100]\n    # 3: [00001011]\n\n    mask = 2 ** bits - 1\n\n    for i in range(uint8tensor.shape[0]):\n        for j in range(num_steps):\n            unpacked_tensor[unpacked_idx] |= uint8tensor[i] &gt;&gt; (bits * j)\n            unpacked_idx += 1\n\n    unpacked_tensor &= mask\n    return unpacked_tensor\n\nunpacked_tensor = torch.tensor([177, 255], \n                               dtype=torch.uint8)\n# Answer should be: torch.tensor([1, 0, 3, 2, 3, 3, 3, 3]\nunpack_weights(unpacked_tensor, 2)\n\ntensor([1, 0, 3, 2, 3, 3, 3, 3], dtype=torch.uint8)\n\n\n\n\nBeyond Linear Qauntization\n\nEmergent features at scale:- Simply some characteristics or features which appear at scale, when model is large.\nFeatures predicted by the model meaning the magnitude of the hidden states started to get large thus making the classic quantization schemes quite obsolete, which led to classic linear quantization algorithms just failing on these models.\nNow how to deal with outlier features for LLMs\nOutlier features simply means hidden states with large magnitude.\nSo there are some interesting papers such as LLM.int8, SmoothQuant, AWQ.\n\nLLM.int8 separates the matmul in two steps:-\n\nFor non-outliers (smaller values)\n\nPerform matmul in int8, then dequantize it.\n\nFor outliers (larger values)\n\nPerform matmul in classical way(basically in the dtype of hidden states usually half precision and then you combine these results)\n\n\nSmoothQuant\n\nApplies A8W8 scheme(quantize weights and activations)\nGiven an input it determines some factor and use it to quantize.\nmigrates the scale variance from activations to weights to reduce the quantization difficulty of activations.\nthe smoothed activation and the adjusted weight are both easy to quantize.\n\nAWQ\n\nUsed a calibration dataset to find out which weights could be responsible of generating outlier features called salient weights.\nand then use that information to scale the weights before quantization and also use that scale during inference to rescale the input as well.\n\n\nRecent SOTA quantization methods:\n\nLL.INT8\nGPTQ\nSmoothQuant\nQLoRA\nAWQ\nQuIP#\nHQQ\nAQLM\n………..\n\nChallenges of Quantization\n\nRetraining (Quantization Aware Training) [less explored]\nLimited Hardware support\nCalibration dataset needed\npacking/unpacking\n\nSome Other resources\n\nMIT Han Lab\nHuggingface transformers quantization docs/blogposts\nllama.cpp discussions\nReddit LocalLlama"
  },
  {
    "objectID": "posts/Meta Learning By Radek Osmulski Chapter Wise Summary Points/index.html",
    "href": "posts/Meta Learning By Radek Osmulski Chapter Wise Summary Points/index.html",
    "title": "Meta Learning Book Chapter Wise Summary Points",
    "section": "",
    "text": "From not being able to program to deep learning expert\n\nUnderstand the value of Stack Overflow, documentation and how to reach both.\nLearn to use a code editor really well.\nLearn to use git for version control.\nLearn how to use local or cloud VM.\nHow to complete a lecture:-\n\nWatch the lecture\nOpen the notebook and try to find out how all the pieces fit together.\nIf you encounter some new functions then it will be good to know about them.\nTweak the hyperparameters and see how it performs.\nOnce you understand, try to create a new notebook recreating the training pipeline that is demonstrated in lecture.\nNow if everything is working well, then you might want to test the same technique on another dataset of similar type.\n\n\n\n\nTheory vs practice:-\n\nIf you aim to reach the highest echelons of deep learning research, you first and foremost need to be a great practitioner!\nFor best effects, use one cup of theory, one cup of practice. Rinse and repeat\nWe are reading papers, but without training actual models, without the experience of applying what we are learning to real-life problem, we are missing a very important feedback loop.\n\n\n\nProgramming is about what you have to say:-\n\nYour ability as a developer is measured by the utility of the things you can express in a language.\nReading and writing a lot of code is the best way to learn a programming language.\n\n\n\nThe secret of good developers:-\n\nGo for long, uninterrupted sessions to avoid context switching.\n\n\n\nThe best way to improve as a developer:-\n\nYou don’t sharpen your skills with resources, books, or articles. You sharpen your skills with practice. If you want to get better, go do the thing.\nRead and write a lot of code.\n\n\n\nHow to use your tools to achieve a state of flow\n\nYou are in a state of flow when nothing separates you and your work\nThat’s why many programmers spend a lot of time studying their editors\nWe are always somewhere between two extremes (state of flow)\nAll we can do is take small steps and see if it is taking us towards the ideal.\nThis can take our productivity to a whole new level.\n\n\n\nUse reality as your mirror:-\n\nIf you want to live a different life tomorrow than you are living today, you have to put your beliefs to the test.\nThe stronger the emotions involved in a situation, the higher the chance you are not seeing things clearly.\nSuppose you want to post something and you are afraid that people whom I respect will unfollow me or they will say I am stupid, but that doesn’t matter in the long run.\nPost it and you will learn on the way.\nThinking about writing something isn’t doing the thing.\n\n\n\nDo genuine work (it compounds):-\n\nReading a book without taking notes is like discovering a new territory and forgetting to draw a map.\nGenuine work moves the atoms in the universe.\nAlways make notes.\nShare whatever you have made.\nThe more atoms you move the better the feedback you receive and the greater the extent to which you can reflect on what you are learning.\n\n\n\nThe hidden game of machine learning:-\n\nAt the core of machine learning lies the ability to generalize to unseen data.\nHow (and why) to create a good validation set blog post by Rachel Thomas\nLearning From Data: A short course by Yaser S. Abu-Mostafa\nBuilding Machine Learning Powered Applications: Going from Idea to Product by Emmanuel Ameisen\n\n\n\nHow to structure a machine learning project:-\n\nThe main condition for a healthy machine learning project is a good train-val-test split.\nAlways have a baseline to get to know whether i am going in right direction or not or what can be done next.\nDon’t run experiments just for the sake of tweaking hyperparameters, instead invest time in exploring other architectures, developing diagnostic code.\nThe idea is to always move in small increments, using simpler models as a stepping stone to more complex ones.\nDefine loss and implement it on simple model consisting of few layers and then check by passing a batch if output is not all zeros and the output shape is required shape.\nCreate a subset of train data(1%) to check whether the modified code is working and the we are going in the right direction.\nRead as much as possible related to the domain you are working on so that our mind can work on the solution and analyze what to do next.\nGo slowly and give attention to each component of the pipeline and grow them systematically.\n\n\n\nHow to win at Kaggle:-\n\nJoin the competition early.\nLearn about the problem domain and data\nWrite diagnostic code and identify where model is struggling.\nFollow Kaggle forums and read daily.\nTry to find out the relevant research papers and see if it is useful in solving the problem.\nYou can read blogs related to the problem you are solving.\nWhile early in the competition, try on different architectures instead of tweaking the hyperparameters.\nHow do we know that we are moving in the right direction? So first of all create a baseline and make a submission. The submission can consist of all zeros.\nTry to find out a validation split that will track the public leaderboard.\nA proper validation split lies at the heart of any good Kaggle submission.\nTo know that if we are going in the right direction, see if the local results matches with the leaderboard. If so then you are going in the right direction and if not then try to find out other ways of splitting the data or something else.\nYou can do ensembling also. the idea is to cancel out the errors from individual model in a hope that their errors are not correlated.\nA related technique is training with cross-validation, where you train multiple models withholding different parts of the data for validation and then combine the results from them.\n\n\n\nThe best hardware for deep learning:-\n\nFollow this blog to know more about GPUs\n\n\n\nDebugging with ease is a superpower:-\n\nRadek Osmulski’s Favorite recent jupyter notebook discovery - the %debug magic:\n\n\n\nTime yourself:-\n\nAlways keep track of the time which you devote to various activities.\n%%timeit\n\n\n\nYou can’t learn a profession by studying a textbook:-\n\nExtensive practice is very important alongside theory.\n\n\n\nOn finding a job:-\n\nThe best approach is to showcase your work, by helping others.\nReach out to the people who know you and can help you.\n\n\n\nThe deep learning party is on Twitter:-\n\nyou will find all the amazing people there\n\n\n\nShare your work:-\n\nMaking Peace with Personal Branding\n\n\n\nWhen to start sharing your work:-\n\nThe sooner, the better!\nWith every piece that we produce, we hone our communication skills\n\n\n\nI am scared to share my work! Help!\n\nWhere some see failure, others see cheap feedback.\nIf you don’t like something then you can always delete it.\n\n\n\nWhat to focus on in sharing your work:-\n\nSpeak to your experience.\nYou don’t have to share something outside of your experience.\n\n\n\nDon’t lose sight of what is important:-\n\nYou won a Kaggle competition that’s good but never ever forget why you have started.\nYou don’t need followers or likes or even social media to learn.\nNearly all of the best DL engineers I(Radek Osmulski) know are either very quiet or completely silent on Twitter\n\n\n\nMake mental space for what matters:-\n\nPeace of mind is the most important prerequisite for creative work.\nAvoid using social media too much!\n\n\n\nTo engage afterburners, find a mentor:-\n\nFind a mentor, who is good at something you care about.\nThe mentor doesn’t need to know you.\nIn general, we want to get answers from the people who are like us and have gone through that path and the people who are not like us to get to know about more ideas.\nAlways make your message as concise and clear as you can.\n\n\n\nThe biggest regret of fast.ai students:-\n\nI wished I spent more time coding and experimenting and less time studying in a more traditional sense.\n100% learning -&gt; x% doing/100-x% real learning.\nShortest path to understand how something works leads through practice.\n80% doing and 20% reading theory.\n\n\n\nPersistence is everything:-\n\nAt any given moment, as you put in the work, you can barely notice a difference in your life. But the longer you stay the course the more rewarding the journey becomes.\nLearning compounds and you need to give it time before you start seeing the exponential results.\nCombine persistence with community involvement and you cannot be stopped.\n\n\n\nChange is about what not to do:-\n\nCeasing how we have done things up to now is harder than taking on new approach.\nTry to find out the behaviors which are no longer helpful to pursuing your goals and replace them with some meaningful.\n\n\n\nLearning might just be enough\n\nA good strategy towards learning is to observe whether you are getting the results that you are after and if you are not then change your approach.\n\n\n\nMore perspectives on mentoring:-\n\nIf you want to catch the attention of someone you admire give yourself a job working for them.\nLearn in public.\n\nWrite blogs and tutorials and cheatsheets.\nSpeak at meetups and conferences.\nAsk and answer things on Stackoverflow or Reddit. Avoid the walled gardens like Slack and Discord, they’re not public.\nMake Youtube videos or Twitch streams.\nStart a newsletter.\nDraw cartoons\n\nMake the thing you wish you had found when you were learning.\nDon’t judge results by claps or tweets or stars or upvotes, just talk to yourself from 3 months ago.\nby far the biggest beneficiary of you trying to help past you is future you. If others benefit, that’s icing.\nDon’t stop there:\n\nEnjoyed a coding video? Reach out to the speaker/instructor and thank them, and ask questions.\nMake PR’s to libraries you use.\nMake your own libraries no one will ever use.\nClone stuff you like, from scratch, to see how they work.\nTeach workshops.\nGo to conferences and summarize what you learned.\n\nTry your best to be right, but don’t worry when you’re wrong, and let the internet correct you when you are inevitably wrong.\nTalk while you code.\nWhenever someone wants some help, then you should be always ready to help them because these are some of the most in-demand people in tech.\n\n\n\nTap into the power of the community to learn faster\n\nInstead of asking questions only try to answer more.\nJoining community will help you in better communication, learn from other’s.\n\n\n\nEnergize\n\nYoga can help.\nTime-restricted eating and its primary benefit is restore hormonal balance."
  },
  {
    "objectID": "posts/Diffusion-2015/index.html",
    "href": "posts/Diffusion-2015/index.html",
    "title": "Diffusion 2015 Paper Implementation",
    "section": "",
    "text": "The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. Diffusion Process consists of forward and reverse process.\n\n\n\n\nThe equation:-\n\n\\(q(x_t|x_{t-1}) = N(x_t,\\sqrt{1-\\beta_t}x_{t-1},\\beta_t\\mathit{I})\\)\nWhy sqrt of alpha_t \\(N(0,\\sigma^2)\\)\nwhere q is forward process\n\\(\\sqrt{1-\\beta_t}x_{t-1}\\) is the mean\n\\(\\beta_t\\mathit{I}\\) is the variance\n\\(\\beta_t\\) is the scheduler which ranges from 0 to 1. The values are kept low to prevent it from exploding.\n\\(\\alpha_t = 1-\\beta_t\\)\n\\(x_t = \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1}\\)\nAfter performing some reparameterization trick\n\\(\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s\\)\n\\(x_t = \\sqrt{\\bar{\\alpha}_t}x_{0} + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon\\)\nWhen we merge two Gaussians with different variance, \\(N(0,\\sigma_{1}^2I)\\) & \\(N(0,\\sigma_{2}^2I)\\), the new distribution is \\(N(0,(\\sigma_{1}^2 + \\sigma_{2}^2)I)\\)\nThe step sizes are controlled by a variance schedule \\({\\beta_{t}\\varepsilon(0,1)}\\) where \\(t\\varepsilon(1,T)\\)\n\\(\\beta_{1} &lt; \\beta_{2} &lt; \\beta_{T}\\), we can afford a larger update step when the sample is noisier.\nWhy scale down \\(x_{0}\\) and scale up noise -&gt; because we want the combined variance\n\nscaling ensures that variance of the combined signal from \\(x_{t-1}\\) and noise remains consistent preventing the forward process from becoming either too noisy too quickly or retaining too much signal.\nscaling \\(x_{t-1}\\) ensures that each step gradually reduces the influance of the original data, allowing the data to smoothly transition into noise over the sequence of steps.\ndiffusion process aims to gradually transform the data into gaussian distribution. To achieve this, each transition must carefully blend the previous data with gaussian noise.\nWhen we add noise to \\(x_{t-1}\\) to get \\(x_{t}\\), we want the combined variance of the signal and noise to be controlled and not grow uncontrollably.\nWithout scaling, simply adding noise to \\(x_{t-1}\\) would increase the variance of \\(x_{t}\\) at each step, leading to an explosion of variance over time.\n\n\n\n\n\n\n\nThe equation:\n\n\\(p_{\\theta}(x_{t-1}|x_t) = N(x_{t-1}, \\mu_\\theta(x_t,t), \\sum_{\\theta}(x_t,t))\\)\n\\(\\mu_\\theta(x_t,t)\\) is the mean\n\\(\\sum_{\\theta}(x_t,t)\\) is equals to variance (\\(\\sigma_{t}^2\\mathit{I}\\))\n\\(\\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}}(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta}(x_t,t))\\)\nThis gives us:-\n\\(x_{t-1} = N(x_{t-1}, \\frac{1}{\\sqrt{\\alpha_t}}(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta}(x_t,t)), \\sqrt{\\beta_t}\\epsilon)\\)\nwhich we can use to calculate output for a given timestep\n\\(x_{t-1} =  \\frac{1}{\\sqrt{\\alpha_t}}(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta}(x_t,t)) +  \\sqrt{\\beta_t}\\epsilon\\)\nwhere \\(\\epsilon_{\\theta}(x_t,t)\\) is model’s ouput\nThe last iteration of reverse process does not add \\(\\sqrt{\\beta_t}\\epsilon\\) because we wouldn’t be able to remove it.\n\n\n\n\n\n\nNoise Addition in Forward Process is Stochastic\n\nRandom Gaussian noise is added at each step, making it impossible to perfectly reverse without learning the structure of the data.\n\nData Distribution is Complex\n\nThe original data lies in a highly structured, non-linear space. Simply reversing noise doesn’t recover this structure—hence, a learned reverse process is necessary.\n\nNeural Network Learns the Reverse Process\n\nThe neural network learns how to denoise the data at each timestep, approximating the transition probability from noisy to clean data.\n\nReverse Process is Probabilistic, Not Deterministic\n\nThe neural network provides the best estimate for removing noise, step by step. A direct reverse would fail because it can’t handle uncertainty or data structure.\n\nConditioning on Noisy Data\n\nThe reverse process depends on conditioning each step on the noisy data from the forward process, which the neural network is trained to model.\n\nNeural Network Recovers Lost Information\n\nAs noise increases during the forward process, information is gradually lost. The neural network learns to infer and recover this information during the reverse process.\n\n\n\n\n\n\nNoise is random. Reversing it directly isn’t possible.\nThe network learns to denoise. It handles uncertainty and complexity.\nEach step refines the data, taking it closer to its original form."
  },
  {
    "objectID": "posts/Diffusion-2015/index.html#forward-process",
    "href": "posts/Diffusion-2015/index.html#forward-process",
    "title": "Diffusion 2015 Paper Implementation",
    "section": "",
    "text": "The equation:-\n\n\\(q(x_t|x_{t-1}) = N(x_t,\\sqrt{1-\\beta_t}x_{t-1},\\beta_t\\mathit{I})\\)\nWhy sqrt of alpha_t \\(N(0,\\sigma^2)\\)\nwhere q is forward process\n\\(\\sqrt{1-\\beta_t}x_{t-1}\\) is the mean\n\\(\\beta_t\\mathit{I}\\) is the variance\n\\(\\beta_t\\) is the scheduler which ranges from 0 to 1. The values are kept low to prevent it from exploding.\n\\(\\alpha_t = 1-\\beta_t\\)\n\\(x_t = \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1}\\)\nAfter performing some reparameterization trick\n\\(\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s\\)\n\\(x_t = \\sqrt{\\bar{\\alpha}_t}x_{0} + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon\\)\nWhen we merge two Gaussians with different variance, \\(N(0,\\sigma_{1}^2I)\\) & \\(N(0,\\sigma_{2}^2I)\\), the new distribution is \\(N(0,(\\sigma_{1}^2 + \\sigma_{2}^2)I)\\)\nThe step sizes are controlled by a variance schedule \\({\\beta_{t}\\varepsilon(0,1)}\\) where \\(t\\varepsilon(1,T)\\)\n\\(\\beta_{1} &lt; \\beta_{2} &lt; \\beta_{T}\\), we can afford a larger update step when the sample is noisier.\nWhy scale down \\(x_{0}\\) and scale up noise -&gt; because we want the combined variance\n\nscaling ensures that variance of the combined signal from \\(x_{t-1}\\) and noise remains consistent preventing the forward process from becoming either too noisy too quickly or retaining too much signal.\nscaling \\(x_{t-1}\\) ensures that each step gradually reduces the influance of the original data, allowing the data to smoothly transition into noise over the sequence of steps.\ndiffusion process aims to gradually transform the data into gaussian distribution. To achieve this, each transition must carefully blend the previous data with gaussian noise.\nWhen we add noise to \\(x_{t-1}\\) to get \\(x_{t}\\), we want the combined variance of the signal and noise to be controlled and not grow uncontrollably.\nWithout scaling, simply adding noise to \\(x_{t-1}\\) would increase the variance of \\(x_{t}\\) at each step, leading to an explosion of variance over time."
  },
  {
    "objectID": "posts/Diffusion-2015/index.html#reverse-process",
    "href": "posts/Diffusion-2015/index.html#reverse-process",
    "title": "Diffusion 2015 Paper Implementation",
    "section": "",
    "text": "The equation:\n\n\\(p_{\\theta}(x_{t-1}|x_t) = N(x_{t-1}, \\mu_\\theta(x_t,t), \\sum_{\\theta}(x_t,t))\\)\n\\(\\mu_\\theta(x_t,t)\\) is the mean\n\\(\\sum_{\\theta}(x_t,t)\\) is equals to variance (\\(\\sigma_{t}^2\\mathit{I}\\))\n\\(\\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}}(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta}(x_t,t))\\)\nThis gives us:-\n\\(x_{t-1} = N(x_{t-1}, \\frac{1}{\\sqrt{\\alpha_t}}(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta}(x_t,t)), \\sqrt{\\beta_t}\\epsilon)\\)\nwhich we can use to calculate output for a given timestep\n\\(x_{t-1} =  \\frac{1}{\\sqrt{\\alpha_t}}(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta}(x_t,t)) +  \\sqrt{\\beta_t}\\epsilon\\)\nwhere \\(\\epsilon_{\\theta}(x_t,t)\\) is model’s ouput\nThe last iteration of reverse process does not add \\(\\sqrt{\\beta_t}\\epsilon\\) because we wouldn’t be able to remove it."
  },
  {
    "objectID": "posts/Diffusion-2015/index.html#key-points-for-diffusion-models-reverse-process",
    "href": "posts/Diffusion-2015/index.html#key-points-for-diffusion-models-reverse-process",
    "title": "Diffusion 2015 Paper Implementation",
    "section": "",
    "text": "Noise Addition in Forward Process is Stochastic\n\nRandom Gaussian noise is added at each step, making it impossible to perfectly reverse without learning the structure of the data.\n\nData Distribution is Complex\n\nThe original data lies in a highly structured, non-linear space. Simply reversing noise doesn’t recover this structure—hence, a learned reverse process is necessary.\n\nNeural Network Learns the Reverse Process\n\nThe neural network learns how to denoise the data at each timestep, approximating the transition probability from noisy to clean data.\n\nReverse Process is Probabilistic, Not Deterministic\n\nThe neural network provides the best estimate for removing noise, step by step. A direct reverse would fail because it can’t handle uncertainty or data structure.\n\nConditioning on Noisy Data\n\nThe reverse process depends on conditioning each step on the noisy data from the forward process, which the neural network is trained to model.\n\nNeural Network Recovers Lost Information\n\nAs noise increases during the forward process, information is gradually lost. The neural network learns to infer and recover this information during the reverse process."
  },
  {
    "objectID": "posts/Diffusion-2015/index.html#simple-summary",
    "href": "posts/Diffusion-2015/index.html#simple-summary",
    "title": "Diffusion 2015 Paper Implementation",
    "section": "",
    "text": "Noise is random. Reversing it directly isn’t possible.\nThe network learns to denoise. It handles uncertainty and complexity.\nEach step refines the data, taking it closer to its original form."
  },
  {
    "objectID": "pages/education.html",
    "href": "pages/education.html",
    "title": "Education",
    "section": "",
    "text": "(The formal ones may be)\n\nB.Tech in Computer Science and Engineering from Indian Institute of Information Technology Kota (2019 - 23)\nHigh School from Dayawati Modi Academy Varanasi\nCourses/certifications relevant to my subject of interest:\n\nDeep Learning Specialization (Coursera)\nData Analysis with Python (Coursera)\nJava Programming: Solving Problems with Software (Coursera)\nPython for Everybody Specialization (Coursera)\nIntroduction to Data Science in Python (Coursera)\nNatural Language Processing Specialization (Coursera)\nMathematics for Machine Learning and Data Science Specialization (Coursera)\nGenerative AI with Large Language Models (Coursera)\nGenerative Adversarial Networks (GANs) Specialization (Coursera)"
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "About Me",
    "section": "",
    "text": "I work on diffusion models at Samsung R&D Institute Noida. For more details on my work experience, check out my LinkedIn profile.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTimeline\n\n\n\n\n\n\n\n\nPosition\nCompany\nDuration\n\n\n\n\nEngineer\nSamsung R&D Institute Noida\nJan 2024 - Present\n\n\nIntern\nSamsung R&D Institute Noida\nJan 2023 - Dec 2023"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Diffusion 2015 Paper Implementation\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2024\n\n\nRitesh Kumar Maurya\n\n\n\n\n\n\n\n\n\n\n\n\nMeta Learning Book Chapter Wise Summary Points\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2024\n\n\nRitesh Kumar Maurya\n\n\n\n\n\n\n\n\n\n\n\n\nQuantization in Depth\n\n\n\n\n\n\n\n\n\n\n\nMay 24, 2024\n\n\nRitesh Kumar Maurya\n\n\n\n\n\n\n\n\n\n\n\n\nQuantization Fundamentals\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nRitesh Kumar Maurya\n\n\n\n\n\n\n\n\n\n\n\n\nFlow Matching\n\n\n\n\n\nUnderstanding Flow Matching: A Novel Approach to Generative Modeling\n\n\n\n\n\nMar 19, 2024\n\n\nRitesh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ritesh Kumar Maurya",
    "section": "",
    "text": "Hi there 👋. I am Ritesh Kumar Maurya. I work on 🧨 diffusion models at Samsung R&D Institute Noida. Know more about me from here.\nThe structure of this website is inspired by Sayak’s site."
  },
  {
    "objectID": "pages/resources.html",
    "href": "pages/resources.html",
    "title": "Resources",
    "section": "",
    "text": "A bit about my formal education"
  },
  {
    "objectID": "posts/Flow Matching/index.html",
    "href": "posts/Flow Matching/index.html",
    "title": "Flow Matching",
    "section": "",
    "text": "Flow Matching is a novel approach to generative modeling that has gained attention in the field of deep learning. This post will explore the fundamentals of Flow Matching, its advantages over other generative approaches, and its practical applications."
  },
  {
    "objectID": "posts/Flow Matching/index.html#introduction",
    "href": "posts/Flow Matching/index.html#introduction",
    "title": "Flow Matching",
    "section": "",
    "text": "Flow Matching is a novel approach to generative modeling that has gained attention in the field of deep learning. This post will explore the fundamentals of Flow Matching, its advantages over other generative approaches, and its practical applications."
  },
  {
    "objectID": "posts/Flow Matching/index.html#what-is-flow-matching",
    "href": "posts/Flow Matching/index.html#what-is-flow-matching",
    "title": "Flow Matching",
    "section": "What is Flow Matching?",
    "text": "What is Flow Matching?\nFlow Matching is a framework for generative modeling that learns continuous transformations between probability distributions. Unlike other approaches such as GANs or Diffusion Models, Flow Matching directly learns the vector field that transforms one distribution into another."
  },
  {
    "objectID": "posts/Flow Matching/index.html#key-concepts",
    "href": "posts/Flow Matching/index.html#key-concepts",
    "title": "Flow Matching",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nContinuous Transformations: Understanding how Flow Matching creates smooth paths between distributions\nVector Fields: The mathematical foundation behind Flow Matching\nProbability Flow ODEs: How differential equations drive the generative process"
  },
  {
    "objectID": "posts/Flow Matching/index.html#advantages-of-flow-matching",
    "href": "posts/Flow Matching/index.html#advantages-of-flow-matching",
    "title": "Flow Matching",
    "section": "Advantages of Flow Matching",
    "text": "Advantages of Flow Matching\n\nFast sampling compared to diffusion models\nStable training dynamics\nTheoretical guarantees on the learned distributions"
  },
  {
    "objectID": "posts/Flow Matching/index.html#implementation-details",
    "href": "posts/Flow Matching/index.html#implementation-details",
    "title": "Flow Matching",
    "section": "Implementation Details",
    "text": "Implementation Details\n# Example code snippet demonstrating basic Flow Matching concepts\nimport torch\nimport torch.nn as nn\n\nclass FlowMatchingModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Model architecture here\n        \n    def forward(self, x, t):\n        # Forward pass implementation\n        pass"
  },
  {
    "objectID": "posts/Flow Matching/index.html#references",
    "href": "posts/Flow Matching/index.html#references",
    "title": "Flow Matching",
    "section": "References",
    "text": "References\n\nReference papers and implementations\nRelated work in the field\nPractical applications and results"
  },
  {
    "objectID": "posts/Flow Matching/index.html#conclusion",
    "href": "posts/Flow Matching/index.html#conclusion",
    "title": "Flow Matching",
    "section": "Conclusion",
    "text": "Conclusion\nSummary of key points and future directions in Flow Matching research."
  },
  {
    "objectID": "posts/Quantization Fundamentals/index.html",
    "href": "posts/Quantization Fundamentals/index.html",
    "title": "Quantization Fundamentals",
    "section": "",
    "text": "This is completely based on Quantization Fundamentals\nFor the code part, you can checkout this link\nQuantizatio helps to reduce the size of the model with little or no degradation.\n\n\nHandling Big Models\nCurrent model compression technique:-\n\nPruning:-remove connections that do not improve the model.\nKnowledge Distillation:- Train a smaller model(Student) using the original model(Teacher). Cons:-You need to have enough hardware to fit both teacher as weel student both.\n\n\n\nOptions to quantize:-\n\n\n\nFig.1 - A layer of Neural Network.\n\n\n\nYou can quantize weights.\nYou can quantize activations that propagate through the layers of neural network\n\nIdea:Store the parameters of the model in ower precision\n\n\nData Types and Sizes\nInteger\n\nUnsigned Integer (8-bit):- Range is [0, 255] [0, 2n-1] (All 8 bits are used to represent the number)\nSigned Integer (8-bit):- Range is [-128, 127] [-2n-1, 2n-1-1] (7 bits are used to represent the number and the 8th bit represent the sign 0:Positive 1:Negative)\n\n\n\n\nData Type\ntorch.dtype\ntorch.dtype alias\n\n\n\n\n8-bit signed integer\ntorch.int8\n\n\n\n8-bit unsigned integer\ntorch.uint8\n\n\n\n16-bit signed integer\ntorch.int16\ntorch.short\n\n\n32-bit signed integer\ntorch.int32\ntorch.int\n\n\n64-bit signed integer\ntorch.int64\ntorch.long\n\n\n\n\nYou can use below mentioned code to find out the more info\n\n\nimport torch\nprint(torch.iinfo(torch.int8))\n\niinfo(min=-128, max=127, dtype=int8)\n\n\nFloating Point\n3 components in floating point: Sign:- positive/negative (always 1 bit) Exponent(range): impact the representable range of the number Fraction(precision): impact on the precision of the number\n\nFP32, BF16, FP16, FP8 are floating point format with a specific number of bits for exponent and the fraction.\n\n\nFP32\n\n\nSign: 1 bit\nExponent(range): 8 bit\nFraction(precision): 23 bit\nTotal: 32 bit\n\n\nBF16\n\n\nSign: 1 bit\nExponent(range): 8 bit\nFraction(precision): 7 bit\nTotal: 16 bit\n\n\nFP16\n\n\nSign: 1 bit\nExponent(range): 5 bit\nFraction(precision): 10 bit\nTotal: 16 bit\n\nComparison Of Data Types\n\n\n\nData Type\nPrecision\nMaximum\n\n\n\n\nFP32\nBest\n~10+38\n\n\nFP16\nBetter\n~1004\n\n\nBF16\nGood\n~1038\n\n\n\n\n\n\n\nData Type\ntorch.dtype\ntorch.dtype alias\n\n\n\n\n16-bit floating point\ntorch.float16\ntorch.half\n\n\n16-bit brain floating point\ntorch.bfloat16\n\n\n\n32-bit floating point\ntorch.float32\ntorch.float\n\n\n64-bit floating point\ntorch.float64\ntorch.double\n\n\n\n\nimport torch\nprint(\"By default, python stores float data in fp64\")\nvalue = 1/3\ntensor_fp64 = torch.tensor(value, dtype = torch.float64)\ntensor_fp32 = torch.tensor(value, dtype = torch.float32)\ntensor_fp16 = torch.tensor(value, dtype = torch.float16)\ntensor_bf16 = torch.tensor(value, dtype = torch.bfloat16)\n\nprint(f\"fp64 tensor: {format(tensor_fp64.item(), '.60f')}\")\nprint(f\"fp32 tensor: {format(tensor_fp32.item(), '.60f')}\")\nprint(f\"fp16 tensor: {format(tensor_fp16.item(), '.60f')}\")\nprint(f\"bf16 tensor: {format(tensor_bf16.item(), '.60f')}\")\n\nprint(torch.finfo(torch.bfloat16))\n\nBy default, python stores float data in fp64\nfp64 tensor: 0.333333333333333314829616256247390992939472198486328125000000\nfp32 tensor: 0.333333343267440795898437500000000000000000000000000000000000\nfp16 tensor: 0.333251953125000000000000000000000000000000000000000000000000\nbf16 tensor: 0.333984375000000000000000000000000000000000000000000000000000\nfinfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)\n\n\nPyTorch Downcasting\n\nwhen a higher data type converted to a lower data type, it results in loss of data\nAdavantages:\n\nReduced memory footprint\nIncreased compute and speed (Depends on the hardware)\n\nDisadvantages:\n\nLess precise\n\nUse case:\n\nMixed precision training\n\nDo computation in smaller precision (FP16/BF16/FP8)\nStore and update the wights in higher precision (FP32)\n\n\n\n\n\nLoading Models by data type\n\ntarget_dtype = torch.float16 or torch.bfloat16\nmodel = model.to(target_dtype)\nmodel = model.half() for fp16\nmodel = model.bfloat16() for bfloat16\nAlways use bfloat16 instead of float16 while using pytorch-cpu\nFP32 is default in pytorch\nmodel.get_memory_footprint()/1e+6\ntorch.set_default_dtype(desired_dtype) # By doing so we can directly load the model in desired dtype without loading in full precision and then quantizing it\nset it back to float32 to avoid unnecesary behaviors\n\n\n\nQuantization Theory\n\nQuantization refers to the process of mapping a large set to a smaller set of values.\nHow do we convert the FP32 weights to INT8 without losing too much information??\n\nIt is done using a linear mapping using linear mapping parameters.\ns = scale\nz = zero point\n\nHow do we get back our original tensor from the quantized tensor?\n\nWe can’t get exactly the original tensor but using dequantization following linear relationship that used to quantize the original tensor.\n\n\n\n\n\nFig.2 - Comparison of tensors.\n\n\nQuantize Using Quanto Library\n\nfrom quanto import quantize, freeze\nquantize(model, weights = desired_dtype, activations = desired_dtype)\nfreeze(model)\nquantize create an intermediate state of the model\nafter calling freeze, we get the quantized weights\n\nUses of the Intermediate State\n\nCalibration\n\nCalibrate model when quantizing the activations of the model.\n\nRange of activation values depends on what input was given. (e.g. different input text will generate different activations)\nMin/Max of activation ranges are used to perform linear quantization.\nHow to get min and max range of activations?\n\nGather sample input data.\nRun inference.\nCalculate min/mac of activations\n\n\nResult: better quantized activations\n\nQuantization Aware Training\n\nTraining in a way that controls how the model performs once it is quantized.\nIntermediate state holds both(quantized as well as unquantized weights).\nUse quantized version of model in forward pass (e.g. BF16)\nUpdate original, unquantized version of model weights during back propogation (e.g. FP32)\n\nIn L4 there is a function in helper.py to calculate the model size\n\nLinear Quantization\nEven if it looks very simple, it is used in many SOTA quantization methods:\n\nAWQ: Activation-aware Weight Quantization\nGPTQ: GPT Quantized\nBNB: BitsandBytes Quantization\n\n\n\n\nFig.3 - Range\n\n\n\nSimple idea: linear mapping\nr = s(q-z)\n\nwhere\nr:- original value(e.g. in FP32)\ns:- scale(e.g. in FP32)\nq:- quantized value(e.g. in INT8)\nz:- zero point(e.g. INT8)\n\nHow do we get scale and zero pint??\n\ns = (r_max-r_min)/(q_max-q_min)\nz = int(round(q_min-r_min/s))\n\n\n\n\nQuantization of LLMs\nRecent SOTA quantization methods:-\n\nLLM.INT8 (only 8-bit)\n\nDecomposes the mat-mul in two stages (outlier part in float16 and non-outlier part in int8).\n\nQLoRA (only 4-bit)\n\nQuantize as well as fine-tune the adapters\n\nAWQ\nGPTQ\nSmoothQuant\n\nMore recent SOTA quantization methods for 2-bit quantization\n\nQuIP#\nHQQ\nAQLM\n\nAll are open-source\nSome Quantization Methods require calibration (from above)\nSome Quantization Methods require Adjustments\nMany of these methods were applied to LLMs, but if we want then we can apply to other type of models by making few adjustments to the quantization methods\n\nSome methods can be applied without making adjustments\n\nLinear quantization\nLL.INT8\nQLoRA\nHQQ\n\nOther approaches are data-dependent\nThere are distributors on HuggingFacewhich gives a quantized version of popular models (TheBloke)\nCheckout HuggingFace Open LLM leaderboard to see how these quantized models are performing\nBenefits of fine-tuning a quantized model:\n\nRecover the accuracy from quantization\nTailor your model for specific use-cases and applications\n\nFine tune with Quantization Aware Training (QAT)\n\nNot compatible with Post Training Quantization (PTQ) techniques.\nThe linear quantization method is an example of PTQ.\nPEFT + QLoRA\n\nQLoRa quantizes the pre-trained base weights in 4-bit precision.\nThis matches the precision of the LoRA weights.\nThis allows the model to add the activations of the pre-trained and adapter weights.\nThis sum of the two activations can be fed as the input to the next layer of the network."
  }
]