{
  "hash": "22287cab158beac46f95de554a91d63e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Quantization in Depth\"\nauthor: \"Ritesh Kumar Maurya\"\ndate: \"2024-05-24\"\ncategories: [Optimization]\nimage: \"qat-training-precision.png\"\n---\n\n- This is completely based on [Quantization in Depth](https://learn.deeplearning.ai/courses/quantization-in-depth/lesson/1/introduction)\n\n- For the code part, you can checkout this [link](https://github.com/riteshrm/Quantization-in-Depth-deeplearning.ai-)\n\n### Quantize and De-quantize a tensor\n+ Advantages of Quantization\n  - Smaller model\n  - Speed gains\n    + Memory bandwidth\n    + Faster operations\n      - GEMM: General Matrix Multiply(matrix to matrix multiplication)\n      - GEMV: General Matrix Vector Multiplication (matrix to vector multiplication)\n\n\n+ Challenges of Quantization\n  - Quantization error\n  - Retraining (Quantization Aware Training)\n  - Limited Hardware support\n  - Calibration dataset needed\n  - packing/unpacking\n\n+ getting q:-\n  - r = s(q-z)\n  q = int(round(r/s+z))\n\n::: {#6815bb53 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n#Helper functions to visualize\ndef plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=None):\n    \"\"\"\n    Plot a heatmap of tensors using seaborn\n    \"\"\"\n    sns.heatmap(tensor.cpu().numpy(), ax=ax, vmin=vmin, vmax=vmax, cmap=cmap, annot=True, fmt=\".2f\", cbar=False)\n    ax.set_title(title)\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\n\n\ndef plot_quantization_errors(original_tensor, quantized_tensor, dequantized_tensor, dtype = torch.int8, n_bits = 8):\n    \"\"\"\n    A method that plots 4 matrices, the original tensor, the quantized tensor\n    the de-quantized tensor and the error tensor.\n    \"\"\"\n    # Get a figure of 4 plots\n    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n\n    # Plot the first matrix\n    plot_matrix(original_tensor, axes[0], 'Original Tensor', cmap=ListedColormap(['white']))\n\n    # Get the quantization range and plot the quantized tensor\n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    plot_matrix(quantized_tensor, axes[1], f'{n_bits}-bit Linear Quantized Tensor', vmin=q_min, vmax=q_max, cmap='coolwarm')\n\n    # Plot the de-quantized tensors\n    plot_matrix(dequantized_tensor, axes[2], 'Dequantized Tensor', cmap='coolwarm')\n\n    # Get the quantization errors\n    q_error_tensor = abs(original_tensor - dequantized_tensor)\n    plot_matrix(q_error_tensor, axes[3], 'Quantization Error Tensor', cmap=ListedColormap(['white']))\n\n    fig.tight_layout()\n    plt.show()\n\ndef linear_q_with_scale_and_zero_point(\n    tensor, scale, zero_point, dtype = torch.int8):\n\n    scaled_and_shifted_tensor = tensor / scale + zero_point\n\n    rounded_tensor = torch.round(scaled_and_shifted_tensor)\n\n    q_min = torch.iinfo(dtype).min\n    q_max = torch.iinfo(dtype).max\n\n    q_tensor = rounded_tensor.clamp(q_min,q_max).to(dtype)\n    \n    return q_tensor\n\ntest_tensor=torch.tensor(\n    [[191.6, -13.5, 728.6],\n     [92.14, 295.5,  -184],\n     [0,     684.6, 245.5]])\n\nscale = 3.5\nzero_point = -70\n\nquantized_tensor = linear_q_with_scale_and_zero_point(\n    test_tensor, scale, zero_point)\n\ndef linear_dequantization(quantized_tensor, scale, zero_point):\n    return scale * (quantized_tensor.float() - zero_point)\n\ndequantized_tensor = linear_dequantization(\n    quantized_tensor, scale, zero_point)\n\nplot_quantization_errors(test_tensor, quantized_tensor,\n                         dequantized_tensor)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=1430 height=374}\n:::\n:::\n\n\n### Get the Scale and Zero-Point\n\n+ s = (r_max-r_min)[current_tensor_range]/(q_max-q_min)[datatype_range]\n+ z = int(round(q_min - r_min/s))\n+ z and quantized tensor are of the same type\n+ z is an integer because it represent zero(in the original 'r' range) with an integer in the quantized 'q' range\n+ if z goes out of range:-\n  - z < q_min:-\n    + z = q_min\n  - z > q_max:-\n    + z = q_max\n\n::: {#50398d43 .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\n\ndef get_q_scale_and_zero_point(tensor, dtype=torch.int8):\n    \n    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n    r_min, r_max = tensor.min().item(), tensor.max().item()\n\n    scale = (r_max - r_min) / (q_max - q_min)\n\n    zero_point = q_min - (r_min / scale)\n\n    # clip the zero_point to fall in [quantized_min, quantized_max]\n    if zero_point < q_min:\n        zero_point = q_min\n    elif zero_point > q_max:\n        zero_point = q_max\n    else:\n        # round and cast to int\n        zero_point = int(round(zero_point))\n    \n    return scale, zero_point\n```\n:::\n\n\n### Symmetric vs Asymmetrci Mode\n+ Assymetric Mode:-\n  - map [r_max, r_min] to [q_max, q_min]\n  - This is what we have implemnted above\n+ Symmetric Mode:-\n  - map [-r_max, r_max] to [-q_max, q_max]\n    + where r_max = max(|tensor|)\n\nWe don't need to use zero point(z=0). this happens because the floating point range and the quantized range are symmetric with respect to zero\n<figure style=\"text-align: center;\">\n  <img src=\"im1.png\" style=\"width:80%\">\n  <figcaption></figcaption>\n</figure>\n\nHence, we can simplify the equation to:-\n\n+ q = int(round(r/s))\n+ s = r_max/q_max\n\n::: {#ab03eccd .cell execution_count=3}\n``` {.python .cell-code}\nimport torch\n\ndef get_q_scale_symmetric(tensor, dtype=torch.int8):\n    r_max = tensor.abs().max().item()\n    q_max = torch.iinfo(dtype).max\n\n    # return the scale\n    return r_max/q_max\n\ndef linear_q_symmetric(tensor, dtype=torch.int8):\n    scale = get_q_scale_symmetric(tensor)\n    \n    quantized_tensor = linear_q_with_scale_and_zero_point(tensor,\n                                                     scale=scale,\n                   # in symmetric quantization zero point is = 0    \n                                                    zero_point=0,\n                                                      dtype=dtype)\n    \n    return quantized_tensor, scale\n```\n:::\n\n\n**Trade-off**\n\n+ Utilization of quantized range:\n  - when using asymmetric quantization, the quantized range is fully utilized\n  - When symmetric mode, if the float range is biased towards one side, this will result in a quantized range where a part of the range is dedicated to values that we'll never see.(e.g ReLU where the output is positive)\n+ Simplicity:\n  - Symmetric mode is much simpler compared to asymmetric mode.\n+ Memory: We don't have to store zero-point for symmetric quantization\n\n+ **We use symmetric quantization for 8-bit, but as we go for lower bits such as 2 or 4 bits, we use asyyemtric quantization**\n\n### Finer Granularity for more Precision\n\n+ Different granularities\n  - per tensor\n  - per channel (along an axis)\n  - per group (group n elements together)\n\n+ The more granular quantization is the more precise it will be.\n\n### Per Channel Quantization\n\n+ we usually use per channel quantization in int8\n\n::: {#8d88c430 .cell execution_count=4}\n``` {.python .cell-code}\ndef linear_q_symmetric_per_channel(r_tensor, dim, dtype=torch.int8):\n    \n    output_dim = r_tensor.shape[dim]\n    # store the scales\n    scale = torch.zeros(output_dim)\n\n    for index in range(output_dim):\n        sub_tensor = r_tensor.select(dim, index)\n        scale[index] = get_q_scale_symmetric(sub_tensor, dtype=dtype)\n\n    # reshape the scale\n    scale_shape = [1] * r_tensor.dim()\n    scale_shape[dim] = -1\n    scale = scale.view(scale_shape)\n    quantized_tensor = linear_q_with_scale_and_zero_point(\n        r_tensor, scale=scale, zero_point=0, dtype=dtype)\n   \n    return quantized_tensor, scale\n```\n:::\n\n\n### Per Group Quantization\n\n+ Group n(e.g. 32, 64, 128) elements together and quantize\n\n+ Per group quantization can require a lot of memory\n  - Let's say we want to quantize a tensor in 4-bit and we choose group_size=32, symmetric mode(z=0), and we store the scales in FP16\n  - It means that we actually quantizing the tensor in **4.5 bits** since we have:\n    - 4 bit(each element is stored in 4 bit)\n    - 16/32 bit (scale in 16 bits for every 32 elements)\n\n::: {#263d6eaa .cell execution_count=5}\n``` {.python .cell-code}\ndef linear_q_symmetric_per_group(tensor, group_size,\n                                 dtype=torch.int8):\n    \n    t_shape = tensor.shape\n    assert t_shape[1] % group_size == 0\n    assert tensor.dim() == 2\n    \n    tensor = tensor.view(-1, group_size)\n    \n    quantized_tensor, scale = linear_q_symmetric_per_channel(\n                                tensor, dim=0, dtype=dtype)\n    \n    quantized_tensor = quantized_tensor.view(t_shape)\n    \n    return quantized_tensor, scale\n\ndef linear_dequantization_per_group(quantized_tensor, scale, \n                                    group_size):\n    \n    q_shape = quantized_tensor.shape\n    quantized_tensor = quantized_tensor.view(-1, group_size)\n    \n    dequantized_tensor = linear_dequantization(quantized_tensor, \n                                               scale, 0)\n    \n    dequantized_tensor = dequantized_tensor.view(q_shape)\n    \n    return dequantized_tensor\n```\n:::\n\n\n### Quantizing Weights and Activations for Inference\n\n+ Depending on what we quantize, the storage and the computation are not the same.\n+ W8A32\n  - If weights are quantized but not the activations, then computation is done floating point (FP16,FP32, BF16)\n  - We need to dequantize the weights to perform the floating point computation (cast to float32)\n\n+ W8A8\n  - Both are quantized\n  - Computation is integer based but not **supported by all hardware**\n\n\n### Custom Build an 8-Bit Quantizer\n\n::: {#03fed75b .cell execution_count=6}\n``` {.python .cell-code}\n#W8A16LinearLayer\ndef w8_a16_forward(weight, input, scales, bias=None):\n    \n    casted_weights = weight.to(input.dtype)\n    output = F.linear(input, casted_weights) * scales\n    \n    if bias is not None:\n        output = output + bias\n      \n    return output\n```\n:::\n\n\n::: {#499dd61b .cell execution_count=7}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass W8A16LinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, \n                 bias=True, dtype=torch.float32):\n        super().__init__()\n        \n        self.int8_weights = nn.Parameter(torch.Tensor([0, 1]\n                                     ).to(dtype=torch.int8))\n\ntry:\n    \n    W8A16LinearLayer(1, 1)\n    \nexcept Exception as error:\n    print(\"\\033[91m\", type(error).__name__, \": \", error, \"\\033[0m\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre><span class=\"ansi-bright-red-fg\"> RuntimeError :  Only Tensors of floating point and complex dtype can require gradients </span>\n</pre>\n```\n:::\n\n:::\n:::\n\n\n+ When we create nn.Parameters, pytorch expects that parameter where it's able to compute gradients on it.\n+ The issue is that with PyTorch, you can't explicitly compute gradients on INT8 tensors.\n+ So above code snippet will give an error saying that only tensors of floating point and complex dtype can require gradients.\n\n\n+ So the right approach to save INT8 weights is instead of saving attributes as being an endless parameter, is to call the method called register buffer.\n+ This way instead of storing a parameter, we just store a buffer, meaning we don't need to compute gradients on the tensor.\n+ You can initialize it with whatever dtype you want.\n\n::: {#1626a9ee .cell execution_count=8}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass W8A16LinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, \n                 bias=True, dtype=torch.float32):\n        super().__init__()\n        \n        \n        self.register_buffer(\n            \"int8_weights\",\n            torch.randint(\n                -128, 127, (out_features, in_features), dtype=torch.int8\n            )\n        )\n        \n        self.register_buffer(\"scales\", \n                             torch.randn((out_features), dtype=dtype)) # We are intereseted in inference only\n        \n        if bias:\n            self.register_buffer(\"bias\", \n                                 torch.randn((1, out_features), \n                                             dtype=dtype))\n        \n        else:\n            self.bias = None\n    def forward(self, input):\n        return w8_a16_forward(self.int8_weights, \n                              input, self.scales, self.bias)\n```\n:::\n\n\n**Quantize a Base Model**\n\n::: {#4c2f2317 .cell execution_count=9}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass W8A16LinearLayer(nn.Module):\n    def __init__(self, in_features, out_features, \n                 bias=True, dtype=torch.float32):\n        super().__init__()\n        \n        \n        self.register_buffer(\n            \"int8_weights\",\n            torch.randint(\n                -128, 127, (out_features, in_features), dtype=torch.int8\n            )\n        )\n        \n        self.register_buffer(\"scales\", \n                             torch.randn((out_features), dtype=dtype))\n        \n        if bias:\n            self.register_buffer(\"bias\", \n                                 torch.randn((1, out_features), \n                                             dtype=dtype))\n        \n        else:\n            self.bias = None\n\n    def quantize(self, weights):\n        w_fp32 = weights.clone().to(torch.float32)\n\n        scales = w_fp32.abs().max(dim=-1).values / 127\n        scales = scales.to(weights.dtype)\n\n        int8_weights = torch.round(weights\n                        /scales.unsqueeze(1)).to(torch.int8)\n\n        self.int8_weights = int8_weights\n        self.scales = scales\n    \n    def forward(self, input):\n        return w8_a16_forward(self.int8_weights, \n                              input, self.scales, self.bias)\n\nmodule = W8A16LinearLayer(4, 8)\nprint(\"Weights before:\\n\" , module.int8_weights)\nrandom_matrix = torch.randn((4, 8), dtype=torch.bfloat16)\nmodule.quantize(random_matrix)\nprint(\"Weights After:\\n\" , module.int8_weights)\nprint(\"Average quantiation error:-\",(random_matrix - module.int8_weights \n * module.scales.unsqueeze(1)).abs().mean())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWeights before:\n tensor([[-128,   24,  -55,   18],\n        [  24,  -95,   84,   47],\n        [  84,   41,  -76,   51],\n        [  73,  -22,   38, -126],\n        [-115,  -83,   55,  -76],\n        [  90,  -31,   -8,   64],\n        [ 117,  115,  -45,   34],\n        [ 120,   19,   26, -103]], dtype=torch.int8)\nWeights After:\n tensor([[-127,   -3,  -41,  -80,   -3,   28,   50,   34],\n        [  25,   31, -127,   59,   64,  -98,  -23,   74],\n        [-123,  -49,  -13,   73,   -7,   -2,  102,  127],\n        [   6,   -2, -127,  -69, -124,  -40,   38,   33]], dtype=torch.int8)\nAverage quantiation error:- tensor(0.0025, dtype=torch.bfloat16)\n```\n:::\n:::\n\n\n### Replace PyTorch layers with Quantized Layers\n+ For language models, it better to not quantize the last layer.\n\n::: {#f86cef17 .cell execution_count=10}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef replace_linear_with_target(module, \n                               target_class, module_name_to_exclude):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear) and not \\\n          any([x == name for x in module_name_to_exclude]):\n            old_bias = child.bias\n\n            new_module = target_class(child.in_features, \n                                      child.out_features, \n                                      old_bias is not None, \n                                      child.weight.dtype)\n            setattr(module, name, new_module)\n            if old_bias is not None:\n              getattr(module, name).bias = old_bias\n        else:\n            # Recursively call the function for nested modules\n            replace_linear_with_target(\n                child, target_class, module_name_to_exclude)\n\n\ndef replace_linear_with_target_and_quantize(module, \n                               target_class, module_name_to_exclude):\n    for name, child in module.named_children():\n        if isinstance(child, nn.Linear) and not \\\n        any([x == name for x in module_name_to_exclude]):\n            old_bias = child.bias\n            old_weight = child.weight\n\n            new_module = target_class(child.in_features, \n                                      child.out_features, \n                                      old_bias is not None, \n                                      child.weight.dtype)\n            setattr(module, name, new_module) # current module is replaced by new_module\n\n            getattr(module, name).quantize(old_weight)\n            \n            if old_bias is not None:\n              getattr(module, name).bias = old_bias\n        else:\n            # Recursively call the function for nested modules\n            replace_linear_with_target_and_quantize(child, \n                     target_class, module_name_to_exclude)\n\nclass DummyModel(torch.nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.emb = torch.nn.Embedding(1, 1)\n    # Try with bias\n    self.linear_1 = nn.Linear(1, 1)\n    # Try without bias\n    self.linear_2 = nn.Linear(1, 1, bias=False)\n    # Lm prediction head\n    self.lm_head = nn.Linear(1, 1, bias=False)\n\nmodel_1 = DummyModel()\nmodel_2 = DummyModel()\nreplace_linear_with_target_and_quantize(model_1, W8A16LinearLayer, [\"lm_head\"])\nprint(\"model_1\",model_1)\n\nreplace_linear_with_target_and_quantize(model_2, W8A16LinearLayer, [])\nprint(\"model_2\",model_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmodel_1 DummyModel(\n  (emb): Embedding(1, 1)\n  (linear_1): W8A16LinearLayer()\n  (linear_2): W8A16LinearLayer()\n  (lm_head): Linear(in_features=1, out_features=1, bias=False)\n)\nmodel_2 DummyModel(\n  (emb): Embedding(1, 1)\n  (linear_1): W8A16LinearLayer()\n  (linear_2): W8A16LinearLayer()\n  (lm_head): W8A16LinearLayer()\n)\n```\n:::\n:::\n\n\n### Quantize any Open Source PyTorch Model\n\n::: {#a5ef87be .cell execution_count=11}\n``` {.python .cell-code}\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n\nmodel_id = \"Salesforce/codegen-350M-mono\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, \n                                    torch_dtype=torch.bfloat16, \n                                             low_cpu_mem_usage=True)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\nprint(pipe(\"def hello_world():\", max_new_tokens=20, do_sample=False)[0][\"generated_text\"])\nprint(\"Model before:\\n\\n\", model)\nreplace_linear_with_target_and_quantize(model, \n                                        W8A16LinearLayer, [\"lm_head\"])\n\nprint(\"Model after:\\n\\n\", model)\nprint(pipe(\"def hello_world():\", max_new_tokens=20, \n           do_sample=False)[0][\"generated_text\"])\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\ndef hello_world():\n    print(\"Hello World\")\n\nhello_world()\n\n# íŒŒ\nModel before:\n\n CodeGenForCausalLM(\n  (transformer): CodeGenModel(\n    (wte): Embedding(51200, 1024)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-19): 20 x CodeGenBlock(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): CodeGenAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (qkv_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n        )\n        (mlp): CodeGenMLP(\n          (fc_in): Linear(in_features=1024, out_features=4096, bias=True)\n          (fc_out): Linear(in_features=4096, out_features=1024, bias=True)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nModel after:\n\n CodeGenForCausalLM(\n  (transformer): CodeGenModel(\n    (wte): Embedding(51200, 1024)\n    (drop): Dropout(p=0.0, inplace=False)\n    (h): ModuleList(\n      (0-19): 20 x CodeGenBlock(\n        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (attn): CodeGenAttention(\n          (attn_dropout): Dropout(p=0.0, inplace=False)\n          (resid_dropout): Dropout(p=0.0, inplace=False)\n          (qkv_proj): W8A16LinearLayer()\n          (out_proj): W8A16LinearLayer()\n        )\n        (mlp): CodeGenMLP(\n          (fc_in): W8A16LinearLayer()\n          (fc_out): W8A16LinearLayer()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1024, out_features=51200, bias=True)\n)\ndef hello_world():\n    print(\"Hello World\")\n\n# hello_world()\n\n# def hello_\n```\n:::\n:::\n\n\n+ Above code snippet modifies the model inplace\n+ Also don't try to change the lm_head otherwise it will not give the desired results\n+ All the rounding errors can sum up once you start generating a lot of tokens, until may be all of these errors get super large so that it affects the model's performance\n\n\n### Load your Quantized Weights from HuggingFace Hub\n\n+ The idea is to quantize weights on bigger instance and then push it back to huggingface. So that we don't have to load and quantize again and again.\n+ Then use meta device from pytorch to load the skeleton of the model instead of loading the whole model itself.\n+ Replace the original layers with the quantized layers\n+ Load the quantized weights from huggingfacehub\n\n::: {#ba0071d8 .cell execution_count=12}\n``` {.python .cell-code}\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nmodel_id = \"facebook/opt-125m\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nreplace_linear_with_target_and_quantize(model, \n                             W8A16LinearLayer, \n                                   [\"lm_head\"])\n\nmodel\n\nquantized_state_dict = model.state_dict()\ntorch.save(quantized_state_dict, r\"/home/ritesh/github_repos/riteshrm.github.io/tmp/quantized_state_dict.pth\")\n```\n:::\n\n\n**How to upload on HF**\n\nfrom huggingface_hub import HfApi, create_repo\n\nYOUR_HF_USERNAME = \"\"\nyour_repo_id = f\"{YOUR_HF_USERNAME}/opt-125m-quantized-dlai\"\n\napi = HfApi()\n\ncreate_repo(your_repo_id)\n\napi.upload_file(\n path_or_fileobj=\"quantized_state_dict.pth\",\n path_in_repo=\"quantized_state_dict.pth\",\n repo_id=your_repo_id\n)\n\n::: {#553244ed .cell execution_count=13}\n``` {.python .cell-code}\nimport torch\nfrom transformers import OPTForCausalLM, AutoTokenizer, AutoConfig\n\nmodel_id = \"facebook/opt-125m\"\nconfig = AutoConfig.from_pretrained(model_id)\n\nwith torch.device(\"meta\"):\n  model = OPTForCausalLM(config)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\nfor param in model.parameters():\n  print(param)\n  break\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/ritesh/miniconda3/envs/quarto/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter containing:\ntensor(..., device='meta', size=(50272, 768), requires_grad=True)\n```\n:::\n:::\n\n\n::: {#50078957 .cell execution_count=14}\n``` {.python .cell-code}\nmodel\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\nOPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0-11): 12 x OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)\n```\n:::\n:::\n\n\n::: {#e980dcd5 .cell execution_count=15}\n``` {.python .cell-code}\nreplace_linear_with_target(model, W8A16LinearLayer, [\"lm_head\"])\nmodel\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\nOPTForCausalLM(\n  (model): OPTModel(\n    (decoder): OPTDecoder(\n      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layers): ModuleList(\n        (0-11): 12 x OPTDecoderLayer(\n          (self_attn): OPTAttention(\n            (k_proj): W8A16LinearLayer()\n            (v_proj): W8A16LinearLayer()\n            (q_proj): W8A16LinearLayer()\n            (out_proj): W8A16LinearLayer()\n          )\n          (activation_fn): ReLU()\n          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (fc1): W8A16LinearLayer()\n          (fc2): W8A16LinearLayer()\n          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n)\n```\n:::\n:::\n\n\n**If loading from HF**\n\nfrom huggingface_hub import hf_hub_download\n\nstate_dict_cache_path = hf_hub_download(\n    \"ybelkada/opt-125m-quantized-dlai\",\n    \"quantized_state_dict.pth\"\n)\n\n::: {#2a941e43 .cell execution_count=16}\n``` {.python .cell-code}\nstate_dict = torch.load(r\"/home/ritesh/github_repos/riteshrm.github.io/tmp/quantized_state_dict.pth\")\nmodel.load_state_dict(state_dict, strict=True, assign=True)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_8991/1695582919.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(r\"/home/ritesh/github_repos/riteshrm.github.io/tmp/quantized_state_dict.pth\")\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n<All keys matched successfully>\n```\n:::\n:::\n\n\n::: {#700ae6ef .cell execution_count=17}\n``` {.python .cell-code}\nfrom transformers import pipeline\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\npipe(\"Hello today I am\", max_new_tokens=40)\n\npipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\npipe(\"Hello today I am giving a course about\", max_new_tokens=10)\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n[{'generated_text': \"Hello today I am giving a course about the history of the world's largest oil producer,\"}]\n```\n:::\n:::\n\n\n### Weights Packing\n\n+ Weights packing is important for storing quantized weights, because torch.int4 is not available as of today, so we need to store and load the weights in int8\n+ This is not ideal because:\n  - tensor will occupy 8-bit per datapoint and might add a considerable overhead for large models\n  - There would be no point of quantizing to 2/4 bits becuase we are still using 8-bit\n\n+ So, we need to pack values\n+ Consider a tensor with 4 values each with 2-bit(0,1,2,3) precision but stored in 8-bit\n  - tensor = torch.tensor([1,0,3,2], dtype=torch.uint8)\n  - 1:- 00000001\n  - 0:- 00000000\n  - 3:- 00000011\n  - 2:- 00000010\n  \n+ We can pack all these values into a single 8-bit value as 177\n  - 177:- 10110001\n\n+ Adavantages:-\n  - It reflects the true memory footprint of the quantized weights\nDisadvantages:-\n  - The unpacked tensors need to be a shape with a multiple of 8//nbits\n  - It needs to unpack before performing an operation\n\n### Packing 2-bit Weights\n\n::: {#7e0b2f60 .cell execution_count=18}\n``` {.python .cell-code}\nimport torch\n\n# Example Tensor: [1, 0, 3, 2]\n    # 1 0 3 2 - 01 00 11 10\n\n    # Starting point of packed int8 Tensor\n    # [0000 0000]\n\n    ##### First Iteration Start:\n    # packed int8 Tensor State: [0000 0000]\n    # 1 = 0000 0001\n    # 0000 0001\n    # No left shifts in the First Iteration\n    # After bit-wise OR operation between 0000 0000 and 0000 0001:\n    # packed int8 Tensor State: 0000 0001\n    ##### First Iteration End\n\n    ##### Second Iteration Start:\n    # packed int8 Tensor State: [0000 0001]\n    # 0 = 0000 0000\n    # 0000 0000\n    # 2 left shifts:\n    # [0000 0000] (1 shift)-> 0000 0000 (2 shift)-> 0000 0000\n    # After bit-wise OR operation between 0000 0001 and 0000 0000:\n    # packed int8 Tensor State: 0000 0001\n    ##### Second Iteration End\n\n    ##### Third Iteration Start:\n    # packed int8 Tensor State: [0000 0001]\n    # 3 = 0000 0011\n    # 0000 0011\n    # 4 left shifts:\n    # [0000 0011] (1 shift)-> 0000 0110 (2 shift)-> 0000 1100\n    # 0000 1100 (3 shift)-> 0001 1000 (4 shift)-> 0011 0000\n    # After bit-wise OR operation between 0000 0001 and 0011 0000:\n    # packed int8 Tensor State: 0011 0001\n    ##### Third Iteration End\n\n    ##### Fourth Iteration Start:\n    # packed int8 Tensor State: [0011 0001]\n    # 2 = 0000 0010\n    # 0000 0010\n    # 6 left shifts:\n    # [0000 0010] (1 shift)-> 0000 0100 (2 shift)-> 0000 1000\n    # 0000 1000 (3 shift)-> 0001 0000 (4 shift)-> 0010 0000\n    # 0010 0000 (5 shift)-> 0100 0000 (6 shift)-> 1000 0000\n    # After bit-wise OR operation between 0011 0001 and 1000 0000:\n    # packed int8 Tensor State: 1011 0001\n    ##### Fourth Iteration End\n\n    # Final packed int8 Tensor State: [1011 0001]\n\ndef pack_weights(uint8tensor, bits):\n    if uint8tensor.shape[0] * bits % 8 != 0:\n        raise ValueError(f\"The input shape needs to be a mutiple \\\n        of {8 / bits} - got {uint8tensor.shape[0]}\")\n\n    num_values = uint8tensor.shape[0] * bits // 8\n\n    num_steps = 8 // bits\n\n    unpacked_idx = 0\n\n    packed_tensor = torch.zeros((num_values), dtype=torch.uint8)\n\n    # 1 0 3 2 - 01 00 11 10\n\n    # [0000 0000] -> 0000 0001\n\n    # 0000 0001\n\n    # 0000 0000 - 0000 0000\n\n    # 0000 0011 - 0011 0000 - 0011 0001\n\n    # 1011 0001\n    \n    for i in range(num_values):\n        for j in range(num_steps):\n            packed_tensor[i] |= uint8tensor[unpacked_idx] << (bits * j)\n            unpacked_idx += 1\n    return packed_tensor\nunpacked_tensor = torch.tensor([1, 0, 3, 2], \n                               dtype=torch.uint8)\npack_weights(unpacked_tensor, 2)\nunpacked_tensor = torch.tensor([1, 0, 3, 2, 3, 3, 3, 3], \n                               dtype=torch.uint8)\npack_weights(unpacked_tensor, 2)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\ntensor([177, 255], dtype=torch.uint8)\n```\n:::\n:::\n\n\n### Unpacking 2-Bit Weights\n\n::: {#2df1094e .cell execution_count=19}\n``` {.python .cell-code}\nimport torch\n\n# Example Tensor: [10110001]\n    # Which was Originally: 1 0 3 2 - 01 00 11 10\n\n    # Starting point of unpacked Tensor\n    # [00000000 00000000 00000000 00000000]\n\n    ##### First Iteration Start:\n    # packed int8 Tensor: [10110001]\n    # You want to extract 01 from [101100 01]\n    # No right shifts in the First Iteration\n    # After bit-wise OR operation between 00000000 and 10110001:\n    # [10110001 00000000 00000000 00000000]\n    # unpacked Tensor state: [10110001 00000000 00000000 00000000]\n    ##### First Iteration End\n\n    ##### Second Iteration Start:\n    # packed int8 Tensor: [10110001]\n    # You want to extract 00 from [1011 00 01]\n    # 2 right shifts:\n    # [10110001] (1 shift)-> 01011000 (2 shift)-> 00101100\n    # After bit-wise OR operation between 00000000 and 00101100:\n    # [10110001 00101100 00000000 00000000]\n    # unpacked Tensor state: [10110001 00101100 00000000 00000000]\n    ##### Second Iteration End\n\n    ##### Third Iteration Start:\n    # packed int8 Tensor: [10110001]\n    # You want to extract 11 from [10 11 0001]\n    # 4 right shifts:\n    # [10110001] (1 shift)-> 01011000 (2 shift)-> 00101100\n    # 00101100 (3 shift)-> 00010110 (4 shift)-> 00001011\n    # After bit-wise OR operation between 00000000 and 00001011:\n    # [10110001 00101100 00001011 00000000]\n    # unpacked Tensor state: [10110001 00101100 00001011 00000000]\n    ##### Third Iteration End\n\n    ##### Fourth Iteration Start:\n    # packed int8 Tensor: [10110001]\n    # You want to extract 10 from [10 110001]\n    # 6 right shifts:\n    # [10110001] (1 shift)-> 01011000 (2 shift)-> 00101100\n    # 00101100 (3 shift)-> 00010110 (4 shift)-> 00001011\n    # 00001011 (5 shift)-> 00000101 (6 shift)-> 00000010\n    # After bit-wise OR operation between 00000000 and 00000010:\n    # [10110001 00101100 00001011 00000010]\n    # unpacked Tensor state: [10110001 00101100 00001011 00000010]\n    ##### Fourth Iteration End\n\n    # Last step: Perform masking (bit-wise AND operation)\n    # Mask: 00000011\n    # Bit-wise AND operation between \n    # unpacked Tensor and 00000011\n    # [10110001 00101100 00001011 00000010] <- unpacked tensor\n    # [00000011 00000011 00000011 00000011] <- Mask\n    # [00000001 00000000 00000011 00000010] <- Result\n\n    # Final\n    # unpacked Tensor state: [00000001 00000000 00000011 00000010]\n\ndef unpack_weights(uint8tensor, bits):\n    num_values = uint8tensor.shape[0] * 8 // bits\n\n    num_steps = 8 // bits\n\n    unpacked_tensor = torch.zeros((num_values), dtype=torch.uint8)\n\n    unpacked_idx = 0\n\n    # 1 0 3 2 - 01 00 11 10\n\n    # [00000000 00000000 00000000 00000000]\n    # [10110001 00101100 00001011 00000010]\n    # [00000001 00000000 00000011 00000010]\n\n    # 10110001\n    # 00000011\n    \n    # 00000001\n\n    # 1: [10110001]\n    # 2: [00101100]\n    # 3: [00001011]\n\n    mask = 2 ** bits - 1\n\n    for i in range(uint8tensor.shape[0]):\n        for j in range(num_steps):\n            unpacked_tensor[unpacked_idx] |= uint8tensor[i] >> (bits * j)\n            unpacked_idx += 1\n\n    unpacked_tensor &= mask\n    return unpacked_tensor\n\nunpacked_tensor = torch.tensor([177, 255], \n                               dtype=torch.uint8)\n# Answer should be: torch.tensor([1, 0, 3, 2, 3, 3, 3, 3]\nunpack_weights(unpacked_tensor, 2)\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\ntensor([1, 0, 3, 2, 3, 3, 3, 3], dtype=torch.uint8)\n```\n:::\n:::\n\n\n### Beyond Linear Qauntization\n\n+ Emergent features at scale:- Simply some characteristics or features which appear at scale, when model is large.\n+ Features predicted by the model meaning the magnitude of the hidden states started to get large thus making the classic quantization schemes quite obsolete, which led to classic linear quantization algorithms just failing on these models.\n\n+ Now how to deal with outlier features for LLMs\n+ Outlier features simply means hidden states with large magnitude.\n\n+ So there are some interesting papers such as LLM.int8, SmoothQuant, AWQ.\n  - LLM.int8 separates the matmul in two steps:-\n    - For non-outliers (smaller values)\n      + Perform matmul in int8, then dequantize it.\n    - For outliers (larger values)\n      + Perform matmul in classical way(basically in the dtype of hidden states usually half precision and then you combine these results)\n\n  - SmoothQuant\n    + Applies A8W8 scheme(quantize weights and activations)\n    + Given an input it determines some factor and use it to quantize.\n    + migrates the scale variance from activations to weights to reduce the quantization difficulty of activations.\n    + the smoothed activation and the adjusted weight are both easy to quantize.\n  \n  - AWQ\n    + Used a calibration dataset to find out which weights could be responsible of generating outlier features called salient weights.\n    + and then use that information to scale the weights before quantization and also use that scale during inference to rescale the input as well.\n\n+ Recent SOTA quantization methods:\n  - LL.INT8\n  - GPTQ\n  - SmoothQuant\n  - QLoRA\n  - AWQ\n  - QuIP#\n  - HQQ\n  - AQLM\n  - ...........\n\n+ Challenges of Quantization\n  - Retraining (Quantization Aware Training)  [less explored]\n  - Limited Hardware support\n  - Calibration dataset needed\n  - packing/unpacking\n  \n+ Some Other resources\n  - MIT Han Lab\n  - Huggingface transformers quantization docs/blogposts\n  - llama.cpp discussions\n  - Reddit LocalLlama\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}