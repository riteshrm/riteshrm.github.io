{
  "hash": "762100128269bb83464719e564bea510",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Quantization Fundamentals\"\nauthor: \"Ritesh Kumar Maurya\"\ndate: \"2024-05-13\"\ncategories: [Optimization]\nimage: \"qat-training-precision.png\"\n---\n\n\n\n\n\n- This is completely based on [Quantization Fundamentals](https://learn.deeplearning.ai/courses/quantization-fundamentals/lesson/1/introduction)\n\n- For the code part, you can checkout this [link](https://github.com/riteshrm/Quantization-Fundamentals-deeplearning.ai-)\n\n- **Quantizatio** helps to reduce the size of the model with little or no degradation. \n\n### Handling Big Models\n\nCurrent model compression technique:-\n\n1. Pruning:-remove connections that do not improve the model.\n\n2. Knowledge Distillation:- Train a smaller model(Student) using the original model(Teacher). Cons:-You need to have enough hardware to fit both teacher as weel student both.\n\n### Options to quantize:-\n\n<figure style=\"text-align: center;\">\n  <img src=\"S1.png\" style=\"width:80%\">\n  <figcaption>Fig.1 - A layer of Neural Network.</figcaption>\n</figure>\n\n1. You can quantize weights.\n\n2. You can quantize activations that propagate through the layers of neural network\n\n\nIdea:Store the parameters of the model in ower precision\n\n### Data Types and Sizes\n\n**Integer**\n\n1. Unsigned Integer (8-bit):- Range is [0, 255]  [0, 2^n^-1] (All 8 bits are used to represent the number)\n\n2. Signed Integer (8-bit):- Range is [-128, 127] [-2^n-1^, 2^n-1^-1] (7 bits are used to represent the number and the 8th bit represent the sign \n0:Positive 1:Negative)\n\n| Data Type | torch.dtype | torch.dtype alias |\n|----------|----------|----------|\n| 8-bit signed integer | torch.int8 |  |\n| 8-bit unsigned integer | torch.uint8 |  |\n| 16-bit signed integer | torch.int16 | torch.short |\n| 32-bit signed integer | torch.int32 | torch.int |\n| 64-bit signed integer | torch.int64 | torch.long |\n\n+ You can use below mentioned code to find out the more info\n\n::: {#125d6fbe .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nprint(torch.iinfo(torch.int8))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\niinfo(min=-128, max=127, dtype=int8)\n```\n:::\n:::\n\n\n**Floating Point**\n\n3 components in floating point:\n  Sign:- positive/negative (always 1 bit)\n  Exponent(range): impact the representable range of the number\n  Fraction(precision): impact on the precision of the number\n\n+ FP32, BF16, FP16, FP8 are floating point format with a specific number of bits for exponent and the fraction.\n\n\n1. FP32\n  + Sign: 1 bit\n  + Exponent(range): 8 bit\n  + Fraction(precision): 23 bit\n  + Total: 32 bit\n\n2. BF16\n  + Sign: 1 bit\n  + Exponent(range): 8 bit\n  + Fraction(precision): 7 bit\n  + Total: 16 bit\n\n3. FP16\n  + Sign: 1 bit\n  + Exponent(range): 5 bit\n  + Fraction(precision): 10 bit\n  + Total: 16 bit\n\n**Comparison Of Data Types**\n\n| Data Type | Precision | Maximum |\n|----------|----------|----------|\n| FP32 | Best | ~10^+38^|\n| FP16 | Better | ~10^04^ |\n| BF16 | Good | ~10^38^ |\n\n<br/>\n\n| Data Type | torch.dtype | torch.dtype alias |\n|----------|----------|----------|\n| 16-bit floating point | torch.float16 | torch.half |\n| 16-bit brain floating point | torch.bfloat16 |  |\n| 32-bit floating point | torch.float32 | torch.float |\n| 64-bit floating point | torch.float64 | torch.double |\n\n::: {#6fcef867 .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nprint(\"By default, python stores float data in fp64\")\nvalue = 1/3\ntensor_fp64 = torch.tensor(value, dtype = torch.float64)\ntensor_fp32 = torch.tensor(value, dtype = torch.float32)\ntensor_fp16 = torch.tensor(value, dtype = torch.float16)\ntensor_bf16 = torch.tensor(value, dtype = torch.bfloat16)\n\nprint(f\"fp64 tensor: {format(tensor_fp64.item(), '.60f')}\")\nprint(f\"fp32 tensor: {format(tensor_fp32.item(), '.60f')}\")\nprint(f\"fp16 tensor: {format(tensor_fp16.item(), '.60f')}\")\nprint(f\"bf16 tensor: {format(tensor_bf16.item(), '.60f')}\")\n\nprint(torch.finfo(torch.bfloat16))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBy default, python stores float data in fp64\nfp64 tensor: 0.333333333333333314829616256247390992939472198486328125000000\nfp32 tensor: 0.333333343267440795898437500000000000000000000000000000000000\nfp16 tensor: 0.333251953125000000000000000000000000000000000000000000000000\nbf16 tensor: 0.333984375000000000000000000000000000000000000000000000000000\nfinfo(resolution=0.01, min=-3.38953e+38, max=3.38953e+38, eps=0.0078125, smallest_normal=1.17549e-38, tiny=1.17549e-38, dtype=bfloat16)\n```\n:::\n:::\n\n\n**PyTorch Downcasting**\n\n+ when a higher data type converted to a lower data type, it results in loss of data\n\n* Adavantages:\n  + Reduced memory footprint\n  + Increased compute and speed (Depends on the hardware)\n\n* Disadvantages:\n  + Less precise\n\n* Use case:\n  + Mixed precision training\n    - Do computation in smaller precision (FP16/BF16/FP8)\n    - Store and update the wights in higher precision (FP32)\n\n\n### Loading Models by data type\n\n+ target_dtype = torch.float16 or torch.bfloat16\n\n+ model = model.to(target_dtype)\n\n+ model = model.half() for fp16\n+ model = model.bfloat16() for bfloat16\n\n+ Always use bfloat16 instead of float16 while using pytorch-cpu\n\n+ FP32 is default in pytorch\n\n+ model.get_memory_footprint()/1e+6\n\n+ torch.set_default_dtype(desired_dtype) # By doing so we can directly load the model in desired dtype without loading in full precision and then quantizing it\n+ set it back to float32 to avoid unnecesary behaviors\n\n### Quantization Theory\n\n+ Quantization refers to the process of mapping a large set to a smaller set of values.\n\n+ How do we convert the FP32 weights to INT8 without losing too much information??\n  - It is done using a linear mapping using linear mapping parameters.\n  - s = scale\n  - z = zero point\n+ How do we get back our original tensor from the quantized tensor?\n  - We can't get exactly the original tensor but using dequantization following linear relationship that used to quantize the original tensor.\n\n<figure style=\"text-align: center;\">\n  <img src=\"Q1.png\" style=\"width:50%\">\n  <figcaption>Fig.2 - Comparison of tensors.</figcaption>\n</figure>\n\n**Quantize Using Quanto Library**\n\n+ from quanto import quantize, freeze\n+ quantize(model, weights = desired_dtype, activations = desired_dtype)\n+ freeze(model)\n+ quantize create an intermediate state of the model\n+ after calling freeze, we get the quantized weights\n\n**Uses of the Intermediate State**\n\n+ Calibration\n  - Calibrate model when quantizing the activations of the model.\n    + Range of activation values depends on what input was given. (e.g. different input text will generate different activations)\n    + Min/Max of activation ranges are used to perform linear quantization.\n    + How to get min and max range of activations?\n      - Gather sample input data.\n      - Run inference.\n      - Calculate min/mac of activations\n  - Result: better quantized activations\n+ Quantization Aware Training\n  - Training in a way that controls how the model performs once it is quantized.\n  - Intermediate state holds both(quantized as well as unquantized weights).\n  - Use quantized version of model in forward pass (e.g. BF16)\n  - Update original, unquantized version of model weights during back propogation (e.g. FP32)\n\n+ In L4 there is a function in helper.py to calculate the model size\n\n**Linear Quantization**\n\nEven if it looks very simple, it is used in many SOTA quantization methods:\n\n+ AWQ: Activation-aware Weight Quantization\n+ GPTQ: GPT Quantized\n+ BNB: BitsandBytes Quantization\n\n<figure style=\"text-align: center;\">\n  <img src=\"Q2.png\" style=\"width:50%\">\n  <figcaption>Fig.3 - Range</figcaption>\n</figure>\n\n+ Simple idea: linear mapping\n+ r = s(q-z)\n  - where\n  - r:- original value(e.g. in FP32)\n  - s:- scale(e.g. in FP32)\n  - q:- quantized value(e.g. in INT8)\n  - z:- zero point(e.g. INT8)\n\n+ How do we get scale and zero pint??\n  - s = (r_max-r_min)/(q_max-q_min)\n  - z = int(round(q_min-r_min/s))\n\n### Quantization of LLMs\n\nRecent SOTA quantization methods:-\n\n+ LLM.INT8 (only 8-bit)\n  - Decomposes the mat-mul in two stages (outlier part in float16 and non-outlier part in int8).\n+ QLoRA (only 4-bit)\n  - Quantize as well as fine-tune the adapters\n+ AWQ\n+ GPTQ\n+ SmoothQuant\n\nMore recent SOTA quantization methods for 2-bit quantization\n\n+ QuIP#\n+ HQQ\n+ AQLM\n\nAll are **open-source**\n\nSome Quantization Methods require calibration (from above)\n\nSome Quantization Methods require Adjustments\n\nMany of these methods were applied to LLMs, but if we want then we can apply to other type of models by making few adjustments to the quantization methods \n\n+ Some methods can be applied without making adjustments\n  - Linear quantization\n  - LL.INT8 \n  - QLoRA\n  - HQQ\n\n+ Other approaches are data-dependent\n+ There are distributors on HuggingFacewhich gives a quantized version of popular models (TheBloke)\n+ Checkout HuggingFace Open **LLM leaderboard** to see how these quantized models are performing\n\n+ Benefits of fine-tuning a quantized model:\n  - Recover the accuracy from quantization\n  - Tailor your model for specific use-cases and applications\n\n+ Fine tune with Quantization Aware Training (QAT)\n  - Not compatible with Post Training Quantization (PTQ) techniques.\n  - The linear quantization method is an example of PTQ.\n  - [PEFT + QLoRA](https://pytorch.org/blog/finetune-llms/)\n    + QLoRa quantizes the pre-trained base weights in 4-bit precision.\n    + This matches the precision of the LoRA weights.\n    + This allows the model to add the activations of the pre-trained and adapter weights.\n    + This sum of the two activations can be fed as the input to the next layer of the network.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}