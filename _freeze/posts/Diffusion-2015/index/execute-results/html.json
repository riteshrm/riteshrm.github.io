{
  "hash": "0a4df7aca19bef87fb6adde7237e95dd",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Diffusion 2015 Paper Implementation\"\nauthor: \"Ritesh Kumar Maurya\"\ndate: \"2024-09-06\"\ncategories: [Diffusion Models]\nimage: \"2015_paper_results.png\"\n---\n\n\n\n\n\n# What are Diffusion Models?\n+ The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data.\nDiffusion Process consists of forward and reverse process.\n\n## Forward Process\n  + The equation:-\n    -  $q(x_t|x_{t-1}) = N(x_t,\\sqrt{1-\\beta_t}x_{t-1},\\beta_t\\mathit{I})$\n    - Why sqrt of alpha_t $N(0,\\sigma^2)$\n    - where q is forward process \n    - $\\sqrt{1-\\beta_t}x_{t-1}$ is the mean\n    - $\\beta_t\\mathit{I}$ is the variance\n    - $\\beta_t$ is the scheduler which ranges from 0 to 1. The values are kept low to prevent it from exploding.\n    - $\\alpha_t = 1-\\beta_t$\n    - $x_t = \\sqrt{\\alpha_t}x_{t-1} + \\sqrt{1-\\alpha_t}\\epsilon_{t-1}$\n\n    - After performing some reparameterization trick\n\n    - $\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s$\n    - $x_t = \\sqrt{\\bar{\\alpha}_t}x_{0} + \\sqrt{1-\\bar{\\alpha}_t}\\epsilon$\n\t- When we merge two Gaussians with different variance, $N(0,\\sigma_{1}^2I)$ & $N(0,\\sigma_{2}^2I)$, the new distribution is $N(0,(\\sigma_{1}^2 + \\sigma_{2}^2)I)$\n\t- The step sizes are controlled by a variance schedule ${\\beta_{t}\\varepsilon(0,1)}$ where $t\\varepsilon(1,T)$\n\t- $\\beta_{1} < \\beta_{2} < \\beta_{T}$, we can afford a larger update step when the sample is noisier.\n\t- Why scale down $x_{0}$ and scale up noise -> because we want the combined variance\n\t\t+ scaling ensures that variance of the combined signal from $x_{t-1}$ and noise remains consistent preventing the forward process from becoming either too noisy too quickly or retaining too much signal.\n\t\t+ scaling $x_{t-1}$ ensures that each step gradually reduces the influance of the original data, allowing the data to smoothly transition into noise over the sequence of steps.\n\t\t+ diffusion process aims to gradually transform the data into gaussian distribution. To achieve this, each transition must carefully blend the previous data with gaussian noise.\n\t\t+ When we add noise to $x_{t-1}$ to get $x_{t}$, we want the combined variance of the signal and noise to be controlled and not grow uncontrollably.\n\t\t+ Without scaling, simply adding noise to $x_{t-1}$ would increase the variance of $x_{t}$ at each step, leading to an explosion of variance over time.\n\n## Reverse Process\n  + The equation:\n    - $p_{\\theta}(x_{t-1}|x_t) = N(x_{t-1}, \\mu_\\theta(x_t,t), \\sum_{\\theta}(x_t,t))$\n    - $\\mu_\\theta(x_t,t)$ is the mean\n    - $\\sum_{\\theta}(x_t,t)$ is equals to variance ($\\sigma_{t}^2\\mathit{I}$)\n    - $\\mu_\\theta(x_t,t) = \\frac{1}{\\sqrt{\\alpha_t}}(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta}(x_t,t))$\n    - This gives us:-\n    - $x_{t-1} = N(x_{t-1}, \\frac{1}{\\sqrt{\\alpha_t}}(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta}(x_t,t)), \\sqrt{\\beta_t}\\epsilon)$\n    - which we can use to calculate output for a given timestep\n    - $x_{t-1} =  \\frac{1}{\\sqrt{\\alpha_t}}(x_t-\\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}}\\epsilon_{\\theta}(x_t,t)) +  \\sqrt{\\beta_t}\\epsilon$\n    - where $\\epsilon_{\\theta}(x_t,t)$ is model's ouput\n    - The last iteration of reverse process does not add $\\sqrt{\\beta_t}\\epsilon$ because we wouldn't be able to remove it.\n\n## Key Points for Diffusion Models (Reverse Process)\n\n1. **Noise Addition in Forward Process is Stochastic**\n   - Random Gaussian noise is added at each step, making it impossible to perfectly reverse without learning the structure of the data.\n\n2. **Data Distribution is Complex**\n   - The original data lies in a highly structured, non-linear space. Simply reversing noise doesn’t recover this structure—hence, a learned reverse process is necessary.\n\n3. **Neural Network Learns the Reverse Process**\n   - The neural network learns how to denoise the data at each timestep, approximating the transition probability from noisy to clean data.\n\n4. **Reverse Process is Probabilistic, Not Deterministic**\n   - The neural network provides the best estimate for removing noise, step by step. A direct reverse would fail because it can’t handle uncertainty or data structure.\n\n5. **Conditioning on Noisy Data**\n   - The reverse process depends on conditioning each step on the noisy data from the forward process, which the neural network is trained to model.\n\n6. **Neural Network Recovers Lost Information**\n   - As noise increases during the forward process, information is gradually lost. The neural network learns to infer and recover this information during the reverse process.\n\n## Simple Summary:\n- **Noise is random.** Reversing it directly isn’t possible.\n- **The network learns to denoise.** It handles uncertainty and complexity.\n- **Each step refines the data,** taking it closer to its original form.\n\n\n# Importing the required packages\n\n::: {#ba2b7b3a .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_swiss_roll\nfrom tqdm import tqdm\ndtype = torch.float32\ndevice = 'cuda'\n```\n:::\n\n\n# Function to sample data of size batch_size\n\n::: {#9d8300e2 .cell execution_count=2}\n``` {.python .cell-code}\ndef sample_batch(batch_size):\n    data, _ = make_swiss_roll(batch_size)\n    data = data[:,[2, 0]]/10\n    data = data * np.array([1, -1])\n    return torch.from_numpy(data).to(dtype = dtype, device=device)\n```\n:::\n\n\n# MLP model designing for reverse process\n\n::: {#20a6d295 .cell execution_count=3}\n``` {.python .cell-code}\nclass MLP(nn.Module):\n    def __init__(self, T, data_dim=2, hidden_dim=64) -> None:\n        super(MLP, self).__init__()\n\n        self.network_head = nn.Sequential(nn.Linear(data_dim, hidden_dim),\n                                          nn.ReLU(),\n                                          nn.Linear(hidden_dim, hidden_dim),\n                                          nn.ReLU())\n        self.network_tail = nn.ModuleList([nn.Sequential(nn.Linear(hidden_dim, hidden_dim),\n                                                         nn.ReLU(),\n                                                         nn.Linear(hidden_dim, 2*data_dim)) for _ in range(T)])\n        \n    def forward(self, xt, t):\n        h = self.network_head(xt)\n        out = self.network_tail[t](h)\n        mu, var = out.chunk(2, dim=1)\n        std = torch.sqrt(torch.exp(var))\n        return mu, std\n```\n:::\n\n\n# Diffusion Model designing for forward, reverse and sampling process \n\n::: {#dca2030d .cell execution_count=4}\n``` {.python .cell-code}\nclass DiffusionModel():\n    def __init__(self, T, model):\n        self.betas  = (torch.sigmoid(torch.linspace(-18, 10, T))*(3e-1 - 1e-5) + 1e-5).to(device)\n        self.alphas = 1-self.betas\n        self.alphas_bar = torch.cumprod(self.alphas, 0).to(device)\n        self.mlp_model = model\n\n    def forward_process(self, x0, t):\n        t = t-1\n        mu_tmp = x0*torch.sqrt(self.alphas_bar[t])\n        std_tmp = torch.sqrt(1-self.alphas_bar[t])\n        eps = torch.randn_like(x0)\n        xt = mu_tmp + std_tmp*eps\n        std = torch.sqrt(((1-self.alphas_bar[t-1])/(1-self.alphas_bar[t]))*self.betas[t])\n        m1 = torch.sqrt(self.alphas_bar[t-1])*self.betas[t]/(1-self.alphas_bar[t])\n        m2 = torch.sqrt(self.alphas[t])*(1-self.alphas_bar[t-1])/(1-self.alphas_bar[t])\n        mu = m1*x0 + m2*xt\n        return mu, std, xt\n    \n    def reverse_process(self, xt, t):\n        t = t-1\n        mu, std = self.mlp_model(xt, t)\n        eps = torch.randn_like(xt).to(device)\n        return mu, std, mu + eps*std\n    \n    def sample(self, batch_size):\n        xt = torch.randn(batch_size, 2).to(device)\n        samples = [xt]\n        for t in range(40,0,-1):\n            if t != 1: xt = self.reverse_process(xt, t)[-1]\n            samples.append(xt)\n        return samples [::-1]\n    \n    def get_loss(self, x0):\n        t = torch.randint(2, 41, (1,))\n        mu_q, sigma_q, xt = self.forward_process(x0, t)\n        mu_p, sigma_p, xt_minus1 = self.reverse_process(xt, t)\n\n        KL = torch.log(sigma_p) - torch.log(sigma_q) + ((sigma_q**2)+(mu_q-mu_p)**2)/(2*(sigma_p**2)) -1/2\n        K = -KL.mean()\n        loss = -K\n        return loss\n```\n:::\n\n\n# Function to plot the forward and reverse process\n\n::: {#c2a8e95a .cell execution_count=5}\n``` {.python .cell-code}\ndef plot(model, file_name, N):\n\n    x0 = sample_batch(N)\n    samples = model.sample(N)\n    data = [x0, model.forward_process(x0, 20)[-1], model.forward_process(x0, 40)[-1]]\n    for i in range(3):\n        plt.subplot(2,3,1+i)\n        plt.scatter(data[i][:, 0].data.cpu().numpy(), data[i][:, 1].data.cpu().numpy(), alpha=0.1, s=1)\n        plt.xlim([-2, 2])\n        plt.ylim([-2, 2])\n        plt.gca().set_aspect('equal')\n\n        if i==0: plt.ylabel(r'$q(\\mathbf{x}^{(0..T)})$', fontsize = 14)\n        if i==0: plt.title(r'$t=0$', fontsize = 14)\n        if i==1: plt.title(r'$t=\\frac{T}{2}$', fontsize = 14)\n        if i==2: plt.title(r'$t=T$', fontsize = 14)\n\n    time_steps = [0, 20, 40]\n    for i in range(3):\n        plt.subplot(2,3,4+i)\n        plt.scatter(samples[time_steps[i]][:, 0].data.cpu().numpy(), samples[time_steps[i]][:, 1].data.cpu().numpy(), alpha=0.1, s=1, c='r')\n        plt.xlim([-2, 2])\n        plt.ylim([-2, 2])\n        plt.gca().set_aspect('equal')\n\n        if i==0: plt.ylabel(r'$q(\\mathbf{x}^{(0..T)})$', fontsize = 14)\n    #plt.savefig(file_name, bbox_inches='tight')\n    plt.show()\n    plt.close()\n```\n:::\n\n\n# Inference of pretrained model\n\n::: {#821a897f .cell execution_count=6}\n``` {.python .cell-code}\nmlp_model = torch.load('mlp_model_299000.pt').to(device)\nmodel = DiffusionModel(40, mlp_model)\nplot(model,f'inference.png', 5_000)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\rites\\AppData\\Local\\Temp\\ipykernel_11176\\2907671096.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  mlp_model = torch.load('mlp_model_299000.pt').to(device)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-2.png){width=602 height=424}\n:::\n:::\n\n\n# Training loop\n\n::: {#74642120 .cell execution_count=7}\n``` {.python .cell-code}\ndef train(model, optimizer,batch_size, epochs):\n    training_loss = []\n    for ep in tqdm(range(1,epochs)):\n        x0 = sample_batch(batch_size)\n        loss = model.get_loss(x0)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        training_loss.append(loss.item())\n\n        if ep%100==0:\n            torch.save(model.mlp_model, f'models/mlp_model_{ep}.pt')\n            plot(model,f'figs/training_epoch_{ep}.png', 5_000)\n    return training_loss\n\nmlp_model = MLP(40).to(device)\nmodel = DiffusionModel(40, mlp_model)\noptimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-4)\n\n# losses = train(model, optimizer, 128_0000, 100000000)\n\n# plt.plot(losses)\n# plt.savefig(f'figs/training_loss.png')\n# plt.close()\n```\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}